210.0-237.12:  ..
237.12-240.88:  Hello everyone, welcome to CS189,
240.88-245.0:  or welcome to CS289A, if you're a grad student,
245.0-246.24:  machine learning.
246.24-247.96:  My name is Jonathan Schuchak,
247.96-250.32:  I'll be your professor this semester.
250.32-253.56:  The things that you need to know to,
253.56-255.44:  or the things you need to have access to
255.44-258.96:  to be part of this class, even if you're not registered yet,
258.96-262.88:  are the class website, which is up there,
262.88-265.08:  it's public, anyone can read it.
265.08-267.88:  You'll need access to the Ed discussion group
267.88-270.15999999999997:  where we discuss the course,
270.15999999999997-272.84:  and there is an invitation link,
272.84-274.59999999999997:  link from the class webpage,
274.59999999999997-278.08:  you can use that to get to our Ed discussion group immediately.
278.08-281.03999999999996:  The other thing you'll need access to is our grade scope,
281.03999999999996-284.0:  which is where you submit your homeworks.
284.0-287.03999999999996:  The, you will need, if you're not already registered
287.03999999999996-288.96:  in grade scope as some of you are,
288.96-292.88:  you'll need to get the access code to get into grade scope.
292.88-295.8:  The access code will be published sometime
295.8-299.71999999999997:  in the next 24 hours on the Ed discussion group.
299.71999999999997-303.2:  So your procedure is, go visit the class webpage,
303.2-304.48:  read it if you have time,
304.48-308.12:  because it's worth knowing what you need to do this semester.
309.12-311.4:  Join our Ed discussion group,
311.4-313.68:  check the Ed discussion group sometime tomorrow,
313.68-317.04:  and look for the enrollment code for grade scope,
317.04-318.8:  if you haven't already received an email
318.8-320.96:  from grade scope saying you're in.
321.12-323.08:  And even if you're not enrolled in the course,
323.08-324.76:  this will work for you.
324.76-328.23999999999995:  Let me be clear that you do not need access to B courses.
328.23999999999995-331.35999999999996:  If you send me an email asking for access to B courses,
331.35999999999996-333.79999999999995:  the answer is no, I don't have time.
333.79999999999995-337.12:  It's not important that you don't need a B course access.
337.12-339.64:  Once you're enrolled, you will get B course access,
339.64-342.28:  which might matter later in the semester, but not now.
344.35999999999996-347.03999999999996:  The class webpage, one reason to visit it
347.03999999999996-348.96:  is because I have all the readings
348.96-350.79999999999995:  for the semester written down.
350.84000000000003-353.48:  So I am not going to remind you
353.48-356.0:  during lectures of what your readings are.
356.0-358.0:  It's your responsibility, as you know,
358.0-360.08:  an adult attending university, too.
360.08-362.88:  Go to the class webpage, find out what readings
362.88-367.08000000000004:  are expected of you for each week, and actually read them.
367.08000000000004-370.40000000000003:  There will also be, I'll make my lecture notes
370.40000000000003-373.0:  available after classes, but I want to be clear
373.0-376.32:  that the primary purpose of the lecture notes
376.32-378.32:  is so that I know what to say during lecture,
378.32-380.96:  so you don't expect it to read like a textbook.
380.96-383.15999999999997:  But you might find it useful.
384.15999999999997-387.15999999999997:  Homework One is already available from the class webpage.
387.15999999999997-390.6:  You can go right now and download it and get started.
390.6-392.6:  It's due one week from now.
393.6-395.76:  Part of the purpose of Homework One is to make sure
395.76-398.4:  that you're ready to do the rest of the homeworks,
398.4-400.68:  that you're familiar with the things that you need to do
400.68-403.28:  with loading files, writing files,
404.28-408.4:  doing basic machine learning computations on files,
408.4-411.47999999999996:  submitting your answers to Kaggle.com,
411.47999999999996-412.71999999999997:  which I'll talk about later.
414.59999999999997-417.32:  The bad news about Homework One is that we're going to ask you
417.32-422.0:  to do something that uses what's called a support vector
422.0-424.79999999999995:  machine to do classification, but we don't actually have
424.79999999999995-426.76:  time to teach you support vector machines
426.76-428.67999999999995:  until after the homework is due.
428.67999999999995-431.88:  But the good news is you don't actually have to implement
431.88-433.24:  support vector machine.
433.24-436.88:  You're just going to use a built-in support vector library
436.88-438.28:  and call it.
438.28-442.15999999999997:  And so the homework is really focused not around you having
442.15999999999997-444.12:  to know everything about support vector machines.
444.12-449.52:  It's more about learning the basic infrastructure
449.52-452.96:  of things you need to know to do the rest of the homeworks
452.96-454.92:  coming up later this semester.
454.92-457.15999999999997:  So please get started on that early.
457.15999999999997-460.08:  One of the things that you need to do as part of this homework
460.08-461.56:  is Kaggle submissions.
461.56-464.56:  You will be limited to two Kaggle submissions per day.
464.56-467.04:  So if you start the homework the morning it's due,
467.04-470.12:  you only get two Kaggle submissions, which probably isn't enough.
470.12-473.92:  So start early on the coding part of the homework, please.
477.6-479.96:  You'll have lots of questions no doubt this semester.
479.96-482.2:  And other, of course, you're welcome to come to our office
482.2-486.56:  hours and discussion sections, but also our primary platform
486.56-490.36:  for electronic communication is going to be at discussion.
490.36-493.68:  And I want to ask that almost all communication of that form,
493.68-496.52000000000004:  like online should go through at discussion.
496.52000000000004-500.32:  Please resist the urge to send too many emails,
500.32-503.72:  because I want to be clear that I am going to be checking
503.72-506.92:  at discussion far, far more often during the semester
506.92-508.44:  than I'll be checking email.
508.44-511.68:  When I teach a course I really follow the discussion group
511.68-517.12:  closely, and my email gets even worse neglected than usual.
517.16-522.24:  However, sometimes you might have a private matter to discuss.
522.24-524.6:  Sometimes something personal is going on in your life.
524.6-528.28:  You need to talk about, or sometimes you
528.28-532.8:  might want to share a large piece of your code for your homework
532.8-534.76:  in hopes that we can help you.
534.76-538.0:  And it's not appropriate to share large pieces
538.0-539.88:  of your code with the rest of the class.
539.88-542.5600000000001:  In those cases, Ed discussion allows you
542.5600000000001-545.04:  to make private posts.
545.04-548.5999999999999:  And the private posts will be viewable only by me and the TAs.
548.5999999999999-550.92:  And so feel free to use private posts
550.92-553.92:  when it's appropriate for one of those two reasons,
553.92-557.76:  personal issues, or your posting solutions,
557.76-559.9599999999999:  or a lot of code, or something that you're not
559.9599999999999-563.36:  allowed to show to your fellow students.
563.36-566.8399999999999:  But we want to ask you to make most of your Ed discussion posts
566.8399999999999-570.04:  public, because the questions you ask, probably there's
570.04-572.48:  100 other people in the room with the same question.
572.48-575.16:  And if we can answer 50 of them at once,
575.16-578.72:  it benefits everybody in the class.
578.72-581.36:  Now, if you have something that's really, really personal,
581.36-583.6800000000001:  like you don't even want the TAs to know about it,
583.6800000000001-585.6800000000001:  you only want me to know about it.
585.6800000000001-589.36:  Then, of course, it's appropriate to send a request
589.36-591.88:  to my email address, that one there.
591.88-594.22:  But again, I'm kind of bad about checking that,
594.22-596.84:  compared to checking Ed discussion.
596.84-597.6:  Just so you know.
598.0-603.84:  And this class will have discussion sections, which
603.84-605.5600000000001:  are optional.
605.5600000000001-607.9200000000001:  I should mention that the lecture itself,
607.9200000000001-609.88:  personal attendance is optional.
609.88-611.48:  Nobody's taking attendance.
611.48-614.0400000000001:  That every lecture is going to be videotaped.
614.0400000000001-616.08:  You can watch the videos at home if you want.
616.08-618.24:  So attendance in most things is optional,
618.24-620.64:  though you do have to attend the exams.
620.64-623.88:  Having said that, I think it's really good for you
623.88-627.36:  to attend a discussion section in person each week.
627.36-629.08:  The sections are wide open.
629.08-631.16:  Attend any section you want.
631.16-632.6800000000001:  If you want to attend every section,
632.6800000000001-633.76:  you can attend every section.
633.76-635.64:  We don't care.
635.64-638.36:  The sections, as you probably already noticed,
638.36-639.32:  they were done this week.
639.32-640.4:  They start next week.
640.4-643.0:  They'll be on Tuesdays and Wednesdays, starting next Tuesday.
647.0-650.16:  A lot of you are probably really wondering about enrollment.
650.16-653.28:  We currently, as far as I know, we're
653.28-656.08:  going to have room from maximum of 703 students
656.08-657.24:  in this class.
657.24-660.12:  Right now, when I last checked this afternoon,
660.12-664.08:  there were 452 wait listed students.
664.08-670.88:  So we are expecting many people to drop the class.
670.88-673.88:  And I'll say more about why later.
673.88-679.0400000000001:  But this is a class that we realize heavily
679.0400000000001-681.12:  on continuous mathematics.
681.12-682.6400000000001:  And homework, too, is kind of going
682.6400000000001-685.6:  to be your personal self-test of whether you're
685.6-687.16:  ready for this class.
687.16-689.6:  And if you don't have the prerequisites,
689.6-691.16:  that's no shame.
691.16-693.2:  You might want to consider putting off the class
693.2-694.32:  to another semester and getting
694.32-697.32:  more of the prerequisites first.
697.32-701.76:  Hopefully, by the time you finish homework, too, you'll know.
701.76-706.84:  And I should add, the homework, too, deadline date,
706.84-709.6:  is the same as the drop deadline for the class.
709.6-711.1600000000001:  And that's not a coincidence.
711.1600000000001-715.08:  It's there because we want you to be informed about whether
715.08-717.8000000000001:  you're making the right decision to drop or to stay
717.8000000000001-719.96:  in the class.
719.96-723.44:  Typically, what we've experienced in the past is maybe there
723.44-727.76:  are 100 plus drops, or sometimes 200.
727.76-730.08:  So if you're on the wait list, there's still a good chance
730.08-731.6:  of getting in.
731.6-733.72:  I've been teaching this class annually,
733.72-735.96:  most years since 2016.
735.96-739.6800000000001:  And every semester, somehow magically,
739.6800000000001-743.48:  we managed to clear all the wait lists by the time of the drop
743.48-744.12:  date.
744.12-745.92:  But there were hundreds of nervous people
745.92-747.28:  right up to the drop date.
747.28-749.36:  So that's just how it goes.
749.36-751.76:  I can't promise that we'll be able to clear the whole wait list
751.76-752.24:  this year.
752.24-753.52:  I hope so, but I don't know.
753.52-755.92:  We'll see.
755.92-759.12:  The prioritization kind of works like this.
759.12-762.44:  So X grads have the highest priority.
762.44-764.4:  If you were a graduate student in X,
764.4-766.44:  we won't find a way to get you into this class.
766.44-770.04:  So if somehow you're not in the class, let me know.
770.04-774.4399999999999:  The second after that, under grads,
774.4399999999999-779.28:  the class as far as I know is being limited to CS and data
779.28-780.4399999999999:  science students.
780.4399999999999-782.88:  They're the only ones who can even get on the wait list.
782.88-786.0799999999999:  I expect most of those will probably clear.
786.0799999999999-789.8399999999999:  Third in line are grad students who are not in X.
789.8399999999999-791.64:  And I'm sorry about that, but we have
791.64-793.48:  to take care of our own first.
793.48-796.04:  I hope we'll be able to accommodate a lot of non-EX grad
796.04-796.8399999999999:  students.
796.84-801.08:  But at this point, I just can't tell you.
801.08-803.72:  We will be able to admit a small number
803.72-805.88:  of concurrent enrollment students, partly
805.88-808.6800000000001:  because concurrent enrollment students give us extra money
808.6800000000001-811.9200000000001:  we can use to take in more of our own Berkeley students.
811.9200000000001-813.08:  But not a lot.
813.08-817.2:  And I think there's 100 plus concurrent enrollment applications.
817.2-819.88:  We won't be able to take half of those even,
819.88-820.88:  but we'll take some.
820.88-823.2:  We'll see what happens.
823.5200000000001-829.5200000000001:  And if you are hoping to get enrolled in the class,
829.5200000000001-832.6800000000001:  but you're not enrolled yet, you should participate normally.
832.6800000000001-834.8000000000001:  You should submit your homeworks.
834.8000000000001-838.6:  Don't, if you finally get admitted to the class three weeks
838.6-841.44:  from now, and you haven't been doing your homeworks,
841.44-842.24:  that's on you.
842.24-844.76:  We're not going to give you an extension.
844.76-846.84:  So you have all the tools.
846.84-848.84:  You will have all the tools you need through a discussion
848.84-851.6:  and grade scope to participate in the class, even if you're
851.6-852.72:  not enrolled yet.
852.72-856.5600000000001:  So don't worry about that.
856.5600000000001-859.12:  All right, that's the high level logistics.
859.12-862.32:  Do we have questions about that?
862.32-863.1600000000001:  Yes, sir?
863.1600000000001-865.6:  So wait, what's the same for undergrad students?
865.6-868.6:  The wait list is not the same for undergrad students.
868.6-872.64:  And we are manually prioritizing them, as I said,
872.64-877.08:  ex-grads first, undergrad second, non-ex-grads third.
877.08-881.84:  So also, whatever it says is the total enrollments in 189
881.84-884.2800000000001:  or 289-and or classes.berkeley.
884.2800000000001-889.64:  You can't take that seriously because 703 is the maximum number
889.64-894.12:  of students we can enroll in both 189 and 289-a combined.
894.12-899.12:  And we have to manually trade off those two.
899.12-900.96:  The system doesn't do it automatically.
900.96-904.6:  So you cannot take seriously whatever it says
904.6-907.12:  about how many seats are available.
907.12-907.8000000000001:  Other questions?
911.84-920.5600000000001:  I have some good news about textbooks.
920.5600000000001-925.44:  If you're satisfied with online versions or PDF versions,
925.44-929.1600000000001:  your textbooks will be free of charge.
929.1600000000001-934.36:  Having said that, I think that the two textbooks that form
934.36-938.0400000000001:  the main reading for this course are well-written textbooks
938.0400000000001-940.0400000000001:  and that the author is deserving your money.
940.04-945.64:  So if you're inclined to buy a paper copy, then either
945.64-947.8:  these textbooks are both.
947.8-950.4399999999999:  I think that that's a nice thing to do to the authors,
950.4399999999999-952.3199999999999:  who I think have done a lot for us.
952.3199999999999-956.16:  But if you're short on funds, I totally understand.
956.16-959.48:  Just download the PDF files and use them for your readings.
959.48-961.04:  No problem.
961.04-963.4:  And so here are our textbooks in introduction
963.4-967.92:  to statistical learning, elements of statistical learning.
967.92-969.4:  They have some authors in common,
969.4-970.88:  and they have some material in common.
970.88-976.52:  But the class web page will tell you which chapters of these books
976.52-979.12:  or sections I think you should read.
979.12-981.92:  And I like these books.
981.92-985.16:  But you're going to see that some things
985.16-987.3199999999999:  are done a little bit differently in this class,
987.3199999999999-990.0799999999999:  especially these books avoid matrix notation.
990.0799999999999-992.88:  And we're definitely not avoiding matrix notation,
992.88-995.76:  because we want to be able to understand things a little more
995.76-998.84:  deeply in this class.
998.84-1001.64:  We also, of course, this being a computer science class,
1001.64-1003.64:  I'm going to talk more about how some of these things
1003.64-1006.2:  are actually computed, how we actually
1006.2-1009.6800000000001:  compute solutions to a lot of these machine learning problems.
1009.6800000000001-1012.2:  And whereas these books kind of are less about that,
1012.2-1015.1600000000001:  more about the statistical part and the application part.
1015.1600000000001-1017.0400000000001:  But they're very good for the statistical part
1017.0400000000001-1018.08:  and the application part.
1022.24-1026.1200000000001:  Back to the prerequisites for this class.
1029.36-1035.56:  I want to say in advance that we're not formally enforcing
1035.56-1037.1999999999998:  any prerequisites.
1037.1999999999998-1038.76:  And part of the reason for that is because it's
1038.76-1041.8:  more important what you know and not important,
1041.8-1045.4399999999998:  what particular class or reading you learned it from.
1045.4399999999998-1049.04:  But we're going to use a lot of tough continuous math.
1049.04-1050.8:  And you need to be ready.
1050.8-1053.32:  And whether you're ready is maybe not a function
1053.32-1056.36:  of whether you've taken a class, or although that can help a lot.
1056.36-1059.3999999999999:  But whether you actually really understood the class.
1059.3999999999999-1062.0:  So the first thing you'll need to know is vector calculus.
1062.0-1066.4399999999998:  And if you've taken math 53, you're probably in good shape,
1066.4399999999998-1070.84:  though that's not the only way that you could get this prerequisite.
1070.84-1072.76:  The amount of vector calculus you need to know
1072.76-1074.32:  is actually pretty limited.
1074.32-1076.36:  You need to understand gradients, and you
1076.36-1078.4399999999998:  need to understand them really well.
1078.4399999999998-1084.3999999999999:  You need to understand the multivariate chain rule really well.
1084.4-1086.4:  Those are the two things we need.
1086.4-1089.88:  If you don't remember div and curl, we're not going to use those.
1089.88-1091.2:  You don't need those.
1091.2-1092.0400000000002:  You need gradients.
1095.0-1096.3200000000002:  You need the chain rule when there's
1096.3200000000002-1100.3200000000002:  more than one intermediate variable.
1100.3200000000002-1102.3200000000002:  There are two bigger chunks of knowledge, though,
1102.3200000000002-1106.6000000000001:  that you really need to have a good handle on that are.
1106.6000000000001-1107.96:  One is linear algebra.
1107.96-1109.68:  And you need to know a lot more linear algebra
1109.68-1112.24:  than you need to know about gradients.
1112.24-1113.68:  There are many, many different ways
1113.68-1118.68:  you might have gotten that information here at Berkeley.
1118.68-1120.6000000000001:  Here's three possibilities.
1120.6000000000001-1122.3600000000001:  There are other linear algebra courses here
1122.3600000000001-1123.68:  and at other universities.
1123.68-1128.44:  So there's many different ways you could prepare for this.
1128.44-1134.3200000000002:  And also, I want to say, though, that to some degree,
1134.3200000000002-1135.8400000000001:  what's going to matter more for this course
1135.8400000000001-1139.04:  is not can you do the exercises in the algebra
1139.04-1140.3600000000001:  in a linear algebra course?
1140.3600000000001-1141.8:  But did you really understand it?
1141.8-1144.0:  Did you have a geometric understanding
1144.0-1146.72:  of what they taught you in your linear algebra class?
1146.72-1148.72:  And I find not all classes are good for that.
1148.72-1151.04:  When I was undergrad, the linear algebra class I took
1151.04-1152.84:  was absolutely horrible.
1152.84-1155.84:  I learned all kinds of algebra and had zero intuition
1155.84-1158.08:  for any of it, zero geometry.
1158.08-1160.28:  And that did not put me in a good shape.
1160.28-1163.6:  But eventually, I recovered.
1163.6-1166.44:  One thing that I really strongly recommend,
1166.44-1169.32:  and this is linked from the class web page, on YouTube,
1169.32-1172.24:  there's someone who goes by the name,
1172.24-1175.32:  I think three blue, one brown, or something like that.
1175.32-1176.96:  It's linked from the class web page anyway.
1176.96-1179.72:  So if I got it wrong, you can find it there.
1179.72-1183.2:  Has a really great series of videos
1183.2-1185.2:  on the fundamentals of linear algebra
1185.2-1188.48:  that show you the visualization, show you the geometry.
1188.48-1193.1599999999999:  So you really know what all that algebra actually means.
1193.1599999999999-1196.04:  And having that is going to be a huge advantage
1196.04-1198.32:  in understanding what we do in this course.
1198.32-1201.08:  So please watch that if you haven't watched it before.
1201.08-1203.8:  That would be a great place to start prepping right now.
1203.8-1211.24:  Because watch those three blue, one brown videos.
1211.24-1214.52:  I wish I knew a resource as good for learning probability
1214.52-1216.6:  because you need to have a really good understanding
1216.6-1218.6:  of probability as well.
1218.6-1221.36:  Here are some places where you might have learned enough
1221.36-1222.72:  probability for this class.
1222.72-1234.16:  And for all three of these things, there are resources linked
1234.16-1235.32:  from the class web page.
1235.32-1239.2:  If you want to brush up and try to swap in all the things
1239.2-1243.1200000000001:  you've forgotten about these three bits of math.
1243.1200000000001-1247.64:  The last thing that you need to function well in this course
1247.64-1249.8000000000002:  is enough programming experience.
1258.2800000000002-1260.8000000000002:  And the thing to understand here is that although this
1260.8000000000002-1264.16:  is a computer science course, it's an advanced computer science
1264.16-1264.48:  course.
1264.48-1266.8000000000002:  And we assume that everybody coming in
1266.8000000000002-1269.4:  knows how to deal with large programming projects,
1269.4-1273.24:  knows how to debug your own code, knows data structures,
1273.24-1276.2800000000002:  like take and see 61b you're an equivalent.
1276.28-1278.72:  And the most important thing I want to make clear
1278.72-1281.52:  is that the teaching assistants in this class
1281.52-1285.6:  have no obligation to look at your code.
1285.6-1289.6399999999999:  If you have a bug and you can't solve it, well,
1289.6399999999999-1291.6:  if a teaching assistant is nice enough
1291.6-1294.6399999999999:  to help you debug your code, that's a bonus.
1294.6399999999999-1296.2:  You should be extra, extra thankful
1296.2-1298.16:  because that's a really nice teaching assistant.
1298.16-1300.08:  But they're going to be told that they're
1300.08-1302.48:  under no obligation to help you debug your code.
1302.48-1305.16:  This is not an introductory programming class.
1305.16-1309.3200000000002:  So please make sure that you are good at debugging code
1309.3200000000002-1310.4:  if you're going to take this class.
1315.16-1319.0800000000002:  One thing I just want to clarify is not a prerequisite at all
1319.0800000000002-1320.8000000000002:  is CS188.
1320.8000000000002-1323.16:  If you haven't taken the introductory artificial intelligence
1323.16-1324.44:  class, don't worry.
1324.44-1325.0400000000002:  Not a problem.
1331.5600000000002-1334.3600000000001:  Just quickly, how are we going to grade this course?
1334.36-1339.6799999999998:  40% of your grade is going to come from seven homeworks.
1346.6-1350.8:  And these are at least initially all planned
1350.8-1355.6799999999998:  to be due on Wednesday nights at 11.59 p.m.
1355.6799999999998-1360.04:  And in order to help you navigate major problems,
1360.04-1364.0:  we're going to offer you five slip days total cumulative
1364.0-1365.12:  overall the homeworks.
1365.12-1368.56:  So if you submit homework to two days late and homework
1368.56-1373.16:  three, three days late, then no penalty.
1373.16-1375.44:  Once you go over five slip days, your homeworks are just not
1375.44-1376.2:  accepted.
1376.2-1381.84:  So five days is a pretty strong limit.
1381.84-1383.72:  We count by the day.
1383.72-1385.0:  We do not count hours.
1385.0-1387.64:  So if you're one minute late, that counts as one day
1387.64-1390.12:  of slip days used.
1391.08-1396.56:  And if you are in the disabled students program,
1396.56-1398.9199999999998:  probably you're going to have additional extensions
1398.9199999999998-1399.84:  available.
1399.84-1403.32:  And yes, extensions and slip days can be combined
1403.32-1405.36:  if you're in that situation.
1405.36-1409.36:  But one thing I want to make clear is that we do not accept
1409.36-1413.52:  any homework more than five days after it's due, even
1413.52-1416.3999999999999:  with a combination of extensions and slip days,
1416.3999999999999-1418.0:  because we have to grade them sometime.
1420.8-1426.52:  20% of your grade will come from the midterm.
1426.52-1430.36:  And I'm sorry to tell you that I don't yet know what day the midterm
1430.36-1431.4799999999998:  will be on.
1431.4799999999998-1435.1999999999998:  But very likely it will be either the Monday or the Wednesday
1435.1999999999998-1438.0:  before spring break.
1438.0-1440.12:  But we're still waiting to find out when we can actually
1440.12-1444.8799999999999:  get rooms so that we can have alternate seating for the midterm.
1444.8799999999999-1448.36:  The time of the midterm will be sometimes
1448.3999999999999-1450.76:  during 6.30 to 8.30 PM.
1450.76-1453.08:  Probably, though, this is not a promise.
1453.08-1454.9599999999998:  We might have to change that at the last minute.
1454.9599999999998-1457.7199999999998:  So please don't consider that a promise.
1457.7199999999998-1460.28:  But the actual midterm will be 80 minutes long.
1460.28-1463.04:  So you don't actually get two hours.
1463.04-1465.08:  You get 80 minutes.
1465.08-1467.9599999999998:  But if we may, because of the room situation,
1467.9599999999998-1470.4799999999998:  we might have some of you starting the exam at 6.30
1470.4799999999998-1472.7199999999998:  and some of you starting at 7 in a different room.
1475.8-1478.08:  I'll let you know on that discussion as soon
1478.08-1479.84:  as we know when the midterm is going to be.
1479.84-1481.76:  So sorry, we don't have that sorted out yet.
1481.76-1486.1999999999998:  But getting rooms from the university is complicated sometimes.
1486.1999999999998-1490.24:  We do at least know the time, though not the room of the final exam.
1490.24-1495.96:  The campus has announced that it will be on Friday, May 10th,
1495.96-1497.4399999999998:  from 3 to 6 PM.
1500.6799999999998-1503.28:  And I think you will have the full three hours to do that exam.
1508.72-1520.08:  If you're a grad student, and therefore you should be in 289A,
1520.08-1522.56:  it's almost the same except that you also
1522.56-1524.48:  have to do a project.
1524.48-1528.32:  And so you're going to have the same amount of your score
1528.32-1530.52:  coming from the homework in the midterm.
1530.52-1534.1999999999998:  But the final exam will be worth only 20% of your final grade.
1534.1999999999998-1537.4399999999998:  And the other 20% will be the project.
1537.44-1538.8400000000001:  These are a team project.
1538.8400000000001-1543.0:  You're expected to work in a team of two or three students.
1543.0-1546.28:  You're expected to do a project that's related to this class
1546.28-1548.16:  somehow, but we're pretty flexible.
1548.16-1552.76:  And we hope that you will find a project that helps
1552.76-1556.0:  the research of at least one of you on your team
1556.0-1559.56:  or is of great personal interest to at least one of you
1559.56-1561.4:  on your team.
1561.4-1564.24:  And so we'll talk about that more later in the semester.
1567.44-1577.88:  Sadly, I need to talk about cheating.
1577.88-1579.1200000000001:  It's not allowed.
1583.64-1587.24:  But what is allowed is discussion of homework problems.
1587.24-1590.44:  So you can talk about homework problems with each other.
1590.44-1592.64:  Just don't write down your answers for each other.
1597.8-1600.68:  Now, regarding sharing code, I think it's
1600.68-1605.68:  OK to show other students small snippets of code
1605.68-1608.44:  if it will help them if it's with simple things.
1608.44-1611.52:  And indeed, you are allowed to share
1611.52-1615.52:  small snippets of code on a discussion as well
1615.52-1619.3200000000002:  if it doesn't give away too much of the answer in your judgment.
1619.3200000000002-1620.56:  What do I mean by small?
1620.56-1622.92:  Well, probably 10 lines or less.
1622.92-1625.1200000000001:  Unless those 10 lines are the complete solution
1625.12-1627.36:  to a problem in which case maybe that's too much.
1634.2399999999998-1637.32:  But don't share large amounts of code.
1637.32-1639.9199999999998:  If you're going over 10 lines of code, that's a red flag.
1643.8-1647.04:  Now, all homeworks, including, of course,
1647.04-1648.9599999999998:  all the programs in your homeworks,
1648.9599999999998-1651.28:  have to be written individually at the end.
1651.28-1653.76:  I mean, you can discuss math problems with each other.
1653.76-1655.84:  But in the end, you have to write up your own solution.
1667.08-1673.52:  And we're going to actively check code for plagiarism
1673.52-1676.16:  with automatic plagiarism checking software.
1676.44-1686.16:  And if you get hot cheating, we don't just give you a zero.
1686.16-1688.88:  The typical penalty is a large negative score.
1696.5600000000002-1701.0400000000002:  Like typically negative, the equivalent of getting negative 100%
1701.0400000000002-1703.4:  on that particular homework.
1703.4-1705.96:  And so it's going to bring down your other homework scores
1705.96-1707.52:  as well.
1707.52-1712.8400000000001:  Having said that, this is not a limitation on what I can do.
1712.8400000000001-1715.68:  I reserve the right to give an F to anyone
1715.68-1718.44:  caught cheating once, if I think it's egregious.
1727.08-1728.1200000000001:  At my discretion.
1728.84-1740.6:  And if you're caught cheating twice, then that will always be an F, of course.
1751.9599999999998-1753.84:  I haven't had big problems in 189.
1753.84-1757.2399999999998:  But the last time I taught CS61B, we had
1757.24-1760.48:  to punish roughly 100 students for cheating.
1760.48-1762.56:  And it was a very painful experience.
1762.56-1764.68:  And so please don't put me through that again.
1768.6-1772.96:  All right, that's the end of the administrative part
1772.96-1773.68:  of this lecture.
1773.68-1775.36:  Are there any questions?
1775.36-1775.88:  Yeah.
1783.48-1784.76:  Sorry, for the what portion?
1788.24-1790.0:  Probability.
1790.0-1793.36:  So certainly needs to know all the basic probability
1793.36-1796.1200000000001:  about understanding coin flips and things
1796.1200000000001-1798.28:  like that, binomial distributions.
1798.28-1800.36:  We're going to use a lot of continuous distributions,
1800.36-1802.56:  especially the normal distribution.
1802.56-1805.2:  The normal distribution, I think, is badly enough
1805.2-1806.76:  hot that I need to teach it again.
1806.76-1810.08:  So there will be some lecturing on that in this class.
1810.08-1813.08:  But definitely understand as much of the univariate
1813.08-1817.04:  in normal distribution coming in as you can.
1817.04-1819.92:  Understand means and variances very well
1819.92-1823.48:  of continuous distributions.
1823.48-1826.48:  That's the most important part.
1826.48-1827.1599999999999:  Question there.
1833.56-1837.32:  The homework format, we do allow handwritten homeworks
1837.32-1839.08:  if your handwriting is neat.
1839.08-1844.1599999999999:  We like to encourage students to do late hack if they can.
1844.1599999999999-1846.96:  Late hack is one of the crown jewels of computer science
1847.68-1849.96:  and if you haven't learned how to use it,
1849.96-1853.44:  it's a great opportunity to take the time to learn how to use it
1853.44-1856.04:  because that's a fantastic skill to have.
1856.04-1858.2:  But yes, we do accept handwritten homeworks
1858.2-1861.2:  as long as we can read them.
1861.2-1864.96:  With a proviso that with a handwritten homework,
1864.96-1866.76:  if we can't read your handwriting,
1866.76-1869.32:  you don't get to an appeal on that.
1869.32-1873.1200000000001:  It's that's purely our judgment.
1873.1200000000001-1873.64:  Yes.
1877.8-1881.6000000000001:  The internet for programming assignments.
1881.6000000000001-1886.6000000000001:  You should not be copying code that's actually meant
1889.04-1892.4:  to solve the same problem you're solving essentially.
1892.4-1895.8:  I know that people will search for a general clues on just how
1895.8-1899.2:  to program in Python, how to use certain libraries.
1899.2-1901.52:  Of course, you have to do that to get through this class.
1901.52-1904.48:  So that's going to be normal and expected.
1905.48-1910.48:  Just, but you need to write your own algorithms
1910.48-1911.48:  at the end of the day.
1913.08-1917.08:  Other questions up front?
1923.28-1927.24:  I will quickly teach multivariate Gaussian again
1927.24-1930.32:  because as I said, I think it's usually not taught well enough.
1930.32-1934.0:  But I really, if you've never seen it before,
1934.0-1936.64:  that's not a good start for this course.
1936.64-1940.32:  So I hope you've seen multivariate Gaussian's taught
1940.32-1944.2:  at least badly once before you come into this course.
1944.2-1946.48:  I definitely hope that you understand
1946.48-1948.36:  univariate Gaussian's really well.
1950.04-1951.0:  That's what I would say.
1953.96-1955.44:  Other questions?
1955.44-1956.48:  Ah, yeah, over there.
1957.2-1959.48:  I think it's a suggestion for the problem
1959.48-1963.16:  or like, do you have any clues or questions?
1963.16-1964.92:  Oh, no, no.
1964.92-1967.48:  You do not need to know anything about R at all.
1967.48-1972.3600000000001:  You probably want to know Python for this class.
1972.3600000000001-1974.6:  Although we do not have a requirement
1974.6-1976.52:  for what programming language you use,
1976.52-1978.92:  your life's going to be a lot easier if you're using Python
1978.92-1981.56:  because everybody else in the class is.
1981.56-1985.64:  I should mention, I forgot to say, this is a new thing.
1985.64-1992.4:  Just this summer, a new version of this textbook came out
1992.4-1995.8000000000002:  with a fifth author who rewrote all the R applications
1995.8000000000002-1996.96:  in Python.
1996.96-2000.96:  So there is a version of introduction to statistical learning
2000.96-2003.68:  with applications in Python.
2003.68-2006.72:  And you might want to download that version instead.
2006.72-2010.2800000000002:  It's also free on the internet at the same link as this book.
2010.2800000000002-2014.64:  But the readings, the reading assignments, I give you,
2014.64-2016.5200000000002:  the chapter numbers and the section numbers
2016.5200000000002-2019.0400000000002:  are going to be for the R version, sorry.
2019.0400000000002-2022.48:  So you'll have to cross-map from the R version
2022.48-2026.0800000000002:  to the Python versions, know what the correct readings are.
2037.0800000000002-2038.4:  All right.
2038.4-2040.88:  Administrative part over, let's start talking
2040.88-2044.44:  about what this class is about.
2044.44-2048.8:  If I were asked to boil down this class into three phrases,
2048.8-2051.04:  here they are.
2051.04-2059.16:  We are going to think about finding patterns in data.
2059.16-2063.28:  And unlike a statistics class, we're not just doing it
2063.28-2064.76:  for the sake of doing it.
2064.76-2066.92:  We're going to use them to make predictions.
2074.84-2077.16:  How do we find patterns?
2077.16-2082.44:  Well, we're going to have models and statistics
2082.44-2085.28:  that help us recognize and understand those patterns.
2098.2400000000002-2104.2400000000002:  And we're going to learn about optimization algorithms
2104.24-2107.3599999999997:  that actually do the learning of those patterns.
2120.3599999999997-2124.24:  So there you go, machine learning in three pithy phrases.
2124.24-2129.24:  The most important word in those three pithy phrases
2129.24-2132.2:  is the data.
2132.2-2134.04:  Data drives everything else.
2134.04-2140.2:  If you do not have enough data, you really can't learn anything.
2140.2-2143.68:  If you're data sucks, you can't really learn anything.
2143.68-2147.56:  And it's very common to have data that's really not good enough
2147.56-2150.2799999999997:  for the machine learning task you want to do.
2150.2799999999997-2154.84:  But it is amazing what you can do when you have lots of good data.
2154.84-2157.56:  So for instance, one thing that you can do now
2157.56-2161.08:  that people have done is you can download millions
2161.08-2165.36:  of photographs from the internet, people's travel photos,
2165.36-2169.52:  and use them to create a 3D reconstruction of Paris.
2169.52-2172.2:  That's awesome that you can do that.
2172.2-2177.04:  And this has been a big change in machine learning.
2177.04-2181.64:  Because machine learning people up to maybe 1995, 2000
2181.64-2183.64:  didn't have access to that kind of data.
2183.64-2185.36:  And then suddenly they did.
2185.36-2188.52:  And this has changed machine learning in a lot of ways,
2188.52-2192.0:  techniques that were formerly out of fashion,
2192.0-2195.88:  like neural networks, came back into fashion specifically.
2195.88-2198.28:  Because when you have huge amounts of data,
2198.28-2200.52:  neural networks shine.
2200.52-2202.88:  They didn't went back when we could only
2202.88-2205.2:  work on small data problems.
2205.2-2207.84:  Well, partly we reprogramming neural networks wrong too,
2207.84-2210.0:  but that's another story.
2210.0-2213.72:  So the neural network has given us access
2213.72-2217.4:  to huge vast quantities of data of all different kinds.
2217.4-2220.52:  And that's just changed machine learning practice
2220.52-2221.32:  a huge amount.
2231.6800000000003-2236.7200000000003:  The most common task that you'll hear about people doing
2236.7200000000003-2238.84:  with machine learning is called classification.
2242.76-2244.84:  Very famous examples of classification
2244.84-2247.7200000000003:  are your computer looks at a photograph
2247.7200000000003-2251.1200000000003:  and tells you, oh, there's a cat in that photo.
2251.1200000000003-2252.1600000000003:  How do we do that?
2252.1600000000003-2256.76:  Well, let's go back and think about what classification is.
2256.76-2259.04:  It'll take us the whole semester to answer the question,
2259.04-2260.4:  how do we recognize the cat?
2260.4-2262.36:  But let's get started.
2265.2400000000002-2268.7200000000003:  All right, so here's a much simpler than recognizing
2268.7200000000003-2271.0:  a cat classification problem.
2271.0-2273.48:  Here you are a credit card company.
2273.48-2277.92:  And you have data about a bunch of your credit card holders
2277.92-2282.2:  and which ones default on their credit card payments
2282.2-2283.44:  and which ones don't.
2283.44-2289.44:  And so in this figure here, we have a bunch of blue circles
2289.72-2294.2400000000002:  who are the people who are paying their credit cards on time.
2294.2400000000002-2297.0:  And we have a bunch of brown crosses
2297.0-2299.0:  who are the people who defaulted
2299.0-2301.2:  on their credit card payments.
2301.2-2303.7999999999997:  And we have some information about them.
2303.7999999999997-2306.8799999999997:  For each of these people, we know their annual income
2306.8799999999997-2311.8799999999997:  and we know what their average credit card balance
2312.6-2315.96:  is over all their credit cards each month.
2315.96-2318.12:  You can get this information through credit services
2318.12-2319.9199999999996:  and other credit card companies.
2321.04-2322.9199999999996:  And you can also get this information about people
2322.9199999999996-2325.08:  who are applying for credit cards.
2325.08-2328.0:  So someone says, hey, give me a credit card
2328.0-2331.16:  and you say, should I give this person a credit card?
2331.16-2332.7999999999997:  Well, you can ask for their income.
2332.7999999999997-2335.8799999999997:  You can get the information about their other balances
2335.8799999999997-2340.8799999999997:  at other banks and compare it with the people you already know
2341.7599999999998-2344.96:  and make a decision about what you think is the probability
2344.96-2347.56:  that they're gonna default on their credit card debt.
2350.64-2352.96:  So how would you do that?
2352.96-2357.96:  So let's say this green dot here,
2358.7200000000003-2361.12:  that's a new applicant.
2363.08-2364.8:  Says, hey, I want a credit card.
2366.56-2368.56:  Our job in machine learning people,
2368.56-2372.08:  as machine learning people, is to make that decision.
2374.2400000000002-2377.92:  So let me just formalize what we're doing here.
2377.92-2380.44:  First of all, we've collected a bunch of data
2380.44-2384.12:  and that data is typically called training points.
2385.12-2386.6:  Why points?
2386.6-2389.12:  Well, because those are points,
2391.12-2394.12:  we have different information about each person.
2396.12-2399.92:  Their income, their average monthly credit card balance.
2399.92-2402.72:  And we think of those as just coordinates
2402.72-2404.3199999999997:  in some Euclidean space.
2404.3199999999997-2407.16:  And so each person becomes a point.
2407.16-2411.2:  Well, each person becomes a point and a label.
2411.2799999999997-2415.68:  The label is good person who pays on time or defaulter.
2418.3999999999996-2420.0:  So the training points are points
2420.0-2422.3999999999996:  because they have income and balance information,
2422.3999999999996-2425.6:  which gives them a position in two-dimensional space.
2425.6-2430.6:  And they also come with an additional piece of information
2430.6-2432.24:  which we call labels.
2432.24-2434.24:  And the label tells you for each point
2434.24-2438.64:  what class it is in, either reliable debtors.
2442.2-2445.3599999999997:  Or defaulted debtors.
2447.52-2450.2:  Those are the two classes in this particular problem.
2453.08-2458.08:  And just to be clear, both the words labels and class,
2458.16-2460.3999999999996:  those are technical terms here.
2460.3999999999996-2465.2799999999997:  The label is the actual information you're given
2465.2799999999997-2467.8399999999997:  with each person about whether they defaulted or not.
2467.84-2472.84:  The class is one of these two choices in this particular case.
2473.8-2475.84:  Sometimes there are more than two choices,
2475.84-2479.32:  but we often work with two class problems like this.
2483.48-2485.48:  And then we want to do prediction.
2485.48-2488.8:  The whole point of machine learning is usually to do prediction.
2488.8-2492.84:  So in this case, we want to evaluate new applicants.
2492.84-2496.76:  In other words, we want to predict what class they're going
2496.76-2497.6000000000004:  to be in.
2497.6000000000004-2502.32:  They don't have a class yet, or maybe they do, but we don't know.
2502.32-2504.32:  So we try to guess their class.
2509.8-2514.2400000000002:  So I've got this green point up here.
2514.2400000000002-2515.88:  And I have to make a decision.
2515.88-2520.0400000000004:  I can say, I think this person is not going to pay their debt.
2520.0400000000004-2522.6800000000003:  And I say, sorry, we're awful.
2522.6800000000003-2526.6800000000003:  Or you say, I think this person is going to pay the debt.
2526.96-2529.96:  So you say, here is your new credit card, ma'am.
2529.96-2530.96:  Don't tell your husband.
2540.6-2546.72:  So when you're doing that, we could think of several ways
2546.72-2547.8399999999997:  you might do this here.
2547.8399999999997-2554.12:  And the first thing you might notice when you look at the existing debtors
2554.12-2558.08:  is that the ones who have high balances
2558.08-2559.3599999999997:  seem to default a lot.
2559.3599999999997-2562.96:  And the ones who have low balances don't seem to default a lot.
2562.96-2566.48:  So maybe you could say, OK, it looks like maybe 1,300
2566.48-2567.8399999999997:  is the right cutoff.
2567.8399999999997-2570.3599999999997:  Let's say that everybody with a balance over 1,300
2570.3599999999997-2571.4:  will probably default.
2571.4-2574.68:  And everyone, under 1,300 is likely to pay.
2574.68-2578.2799999999997:  And that seems like it predicts a lot of the existing people,
2578.2799999999997-2579.12:  a lot of the time.
2579.12-2582.3199999999997:  So in this case, you'll say, OK, I think
2582.32-2585.6800000000003:  that this green point here is on the left side of that line.
2585.6800000000003-2587.76:  So let's give that person a credit card.
2593.1200000000003-2597.92:  Or I could say, well, I can be more sophisticated than that,
2597.92-2602.36:  because it does look like income has an influence as well.
2602.36-2604.76:  A small influence, but nevertheless,
2604.76-2613.2400000000002:  I think that I can maybe tilt my line a little bit
2613.2400000000002-2616.2000000000003:  and get a slightly more accurate division
2616.2000000000003-2619.6800000000003:  between people who default and people who don't default,
2619.6800000000003-2621.92:  because people with higher incomes,
2621.92-2624.2000000000003:  they seem slightly less likely to default.
2627.1600000000003-2630.2400000000002:  But if I can do that, maybe I might say, OK,
2630.2400000000002-2631.88:  why do I have to use a straight line?
2631.88-2636.6:  Why don't I sort of try to eyeball this
2636.6-2639.6:  so that I get as many blue points on the left side
2639.6-2643.04:  and as many brown crosses on the right side as I can?
2643.04-2646.36:  Maybe that's the best decision boundary.
2646.36-2648.48:  How do I know which is the best choice?
2648.48-2650.52:  Which of those things should I do?
2650.52-2652.7200000000003:  The rest of this lecture will talk about that,
2652.7200000000003-2655.12:  how to distinguish between these choices.
2656.12-2664.24:  But just to draw a slightly simpler example,
2664.24-2676.04:  maybe you have, there's some people who pay on time,
2676.04-2680.7999999999997:  there's some defaulters there, but maybe it's a little hard
2680.7999999999997-2684.24:  to distinguish between them exactly.
2684.24-2688.9199999999996:  And so I have a new point that I want to classify,
2688.9199999999996-2691.9599999999996:  this point here, and say, is that person going to default
2691.9599999999996-2695.64:  or is that person going to pay on time?
2695.64-2699.3599999999997:  Well, one way I could do this, answer this question,
2699.3599999999997-2702.72:  is I could look for their nearest neighbor, I could say,
2702.72-2706.52:  let's look at the training points, the blue and red ones,
2706.52-2709.2799999999997:  and find the training point that is closest
2709.2799999999997-2712.2:  to the query point, the green point,
2712.2-2715.3599999999997:  and assign the same label to the query point.
2715.3599999999997-2718.7999999999997:  So I'll say, OK, the nearest neighbor is blue,
2718.7999999999997-2721.2:  someone who paid on time, so I think that green point
2721.2-2723.3599999999997:  is going to pay on time.
2723.3599999999997-2729.04:  And if you do that, that's called a nearest neighbor classifier.
2729.04-2732.3599999999997:  Or another way I can do it is I could try to draw a decision
2732.3599999999997-2734.8799999999997:  boundary that sort of separates the red points
2734.8799999999997-2741.52:  from the blue points, maybe something like that.
2741.52-2744.28:  But if I do that, well, now the green point
2744.28-2747.84:  is on the right side of that line, along with a bunch
2747.84-2749.0:  of red points.
2749.0-2754.08:  So if I do this classifier, then I might say that that green
2754.08-2756.4:  point looks like it's going to default.
2756.4-2758.08:  So this is a different classifier.
2758.08-2760.52:  It's called a linear classifier.
2760.52-2762.32:  The linear classifier doesn't always
2762.32-2765.48:  make the same predictions as the nearest neighbor classifier.
2765.48-2766.56:  Which one is better?
2766.56-2770.92:  Well, let's think about that.
2770.92-2773.28:  By the way, we're going to learn lots of algorithms
2773.28-2777.6:  for learning linear classifiers over the next several weeks,
2777.6-2778.6:  but not today.
2778.6-2780.28:  So today, what we're going to think about
2780.28-2786.96:  is just what might make one type of classifier better
2786.96-2788.16:  than another.
2788.16-2788.76:  Why?
2788.76-2791.0:  How do you choose?
2791.0-2795.6:  And so let's look at a more complicated data set
2795.6-2799.2400000000002:  with the same two classifiers side by side.
2799.3199999999997-2802.56:  So there we go, nearest neighbor classifier on the left.
2808.16-2810.52:  And let me make clear, again, both these figures
2810.52-2815.3199999999997:  are the same data set, same blue point, same brown points.
2815.3199999999997-2820.56:  The only difference is what type of classifier we're using.
2821.44-2831.4:  And with these two classifiers, you'll see I've drawn some black lines.
2831.4-2833.88:  The black lines are called decision boundaries.
2833.88-2851.92:  Obviously, they're pretty different for the two classifiers,
2851.92-2856.36:  even though they're both potentially good classifiers.
2856.36-2857.92:  All right.
2857.92-2861.96:  So again, at left, we have a nearest neighbor classifier,
2861.96-2865.04:  which classifies a new point by finding the nearest point
2865.04-2868.32:  in the training data and assigning it the same class.
2868.32-2872.28:  And the way this figure works is if you look closely,
2872.28-2874.48:  you'll see there's some background color.
2874.48-2877.32:  I don't know if it's coming very well through on the projector.
2877.32-2879.04:  It's a little better in the lecture notes.
2879.04-2886.32:  But what you might notice is that in this region here,
2886.32-2890.04:  where all the blue points are, the background is also
2890.04-2891.2:  colored blue.
2891.2-2894.24:  And what the background colored blue means
2894.24-2899.3599999999997:  is that if you have a test point, a query point,
2899.3599999999997-2902.3599999999997:  that you want to predict the class of,
2902.3599999999997-2904.3599999999997:  and you find it somewhere in that blue region,
2904.3599999999997-2909.24:  we're going to say, OK, that's a good paying debtor.
2909.24-2913.0:  And if it's in the brown region, then we're going to say,
2913.0-2916.3999999999996:  oh, that's a defaulter.
2916.3999999999996-2918.2:  That's going to be our prediction.
2918.2-2921.16:  The prediction might be wrong, but that's how we predict things.
2921.6-2924.7599999999998:  And over on the right, well, the linear classifier,
2924.7599999999998-2928.3599999999997:  if you look closely, this region up here is colored brown,
2928.3599999999997-2933.3199999999997:  which means any test point that's found in this region,
2933.3199999999997-2935.72:  we're going to predict that it's a defaulter.
2935.72-2938.72:  Any test point that's down here in this blue region,
2938.72-2940.96:  we're going to predict will not default,
2940.96-2942.04:  will pay bills on time.
2942.68-2947.48:  One other subtle difference that probably isn't obvious here,
2947.48-2951.88:  but when we compute a linear decision boundary,
2951.88-2955.48:  this is a linear decision boundary, because it's a straight line.
2955.48-2959.88:  When we compute one, it's usually explicitly computed.
2959.88-2962.88:  We actually compute one.
2962.88-2966.72:  We actually compute one.
2966.72-2974.72:  We actually compute an equation for that line,
2974.72-2977.72:  for that decision boundary.
2977.72-2980.16:  With a nearest neighbor classifier on the other hand,
2980.16-2984.56:  we don't do that, because it's not very useful in practice.
2984.56-2988.8799999999997:  What we actually do is someone gives you a query point.
2988.8799999999997-2990.9199999999996:  You actually compute the distance from that query point
2990.9199999999996-2993.64:  to every training point.
2993.64-2996.3199999999997:  And you take the shortest distance and look
2996.32-2999.4:  at the label of that point, and that's what you return.
2999.4-3004.6800000000003:  So this is not explicitly computed,
3004.6800000000003-3009.1200000000003:  though we did explicitly compute it just to make this figure.
3009.1200000000003-3011.44:  It's an implicit thing that comes out
3011.44-3013.0800000000004:  of the nearest neighbor computation.
3017.84-3020.92:  So if you look at the two of these, and you ask,
3020.92-3024.84:  which do I think is better, one thing that's very clear
3024.84-3027.92:  is that the one on the left at least
3027.92-3031.48:  computes the correct answer for every training point.
3031.48-3033.44:  The one on the right doesn't even do that.
3033.44-3039.0:  The one on the right, there is a blue point right here,
3039.0-3040.76:  but it's on the brown side of the line.
3040.76-3045.52:  So if you feed that point into the linear classifier
3045.52-3049.44:  and say, please predict will that circle default or not?
3049.44-3053.6400000000003:  The linear classifier will predict that circle will default,
3053.64-3057.6:  which is wrong, or at least was wrong for the training point.
3057.6-3061.56:  So you might say, you can't even remember the training data
3061.56-3064.7599999999998:  that we used to teach you.
3064.7599999999998-3067.72:  How can that be a good classifier?
3067.72-3071.16:  Nevertheless, it is entirely possible that the one
3071.16-3072.8799999999997:  on the right is a much better classifier
3072.8799999999997-3074.0:  than the one on the left.
3074.0-3075.24:  Can someone tell me why?
3078.68-3079.64:  What is overfitting mean?
3084.64-3088.0:  OK, but why would it work well on the training data
3088.0-3090.6:  and not work well on the test data?
3090.6-3091.2:  Why?
3091.2-3098.4:  If you want to train data, say that it's a training data,
3098.4-3100.56:  say that it's a training data.
3100.56-3101.56:  Hmm.
3101.56-3102.56:  Oh, yeah.
3102.56-3105.3199999999997:  If it is, I want to do that.
3105.3199999999997-3108.24:  OK, that's a good insight.
3108.24-3110.8799999999997:  If there is noise in the training data,
3110.88-3113.28:  like if there are points that are just
3113.28-3116.6:  have the wrong label because of a data entry error,
3116.6-3120.0:  those wrong points are going to mess up your results.
3120.0-3121.88:  Did you have something you wanted to add?
3121.88-3122.88:  Ma'am.
3122.88-3125.44:  So if it is a problem, why would you
3125.44-3127.88:  use that boundary problem?
3127.88-3130.92:  Like, basically, that is like a problem.
3130.92-3133.56:  And if it is a problem, then you use that problem.
3133.56-3134.4:  Yeah, yeah.
3134.4-3136.7200000000003:  So that's kind of a just salt thing to say,
3136.7200000000003-3138.32:  but you're very right.
3138.32-3139.04:  Look at these.
3139.04-3140.4:  Look at these decision boundaries.
3140.4-3144.64:  Do these look like they would conform
3144.64-3148.6800000000003:  to some real insight about this data?
3148.6800000000003-3150.04:  Probably not.
3150.04-3151.96:  I mean, we can't say for sure.
3151.96-3155.08:  Well, for credit card debtors, we can say for sure.
3155.08-3157.64:  There's no way that those little bumps have meaning
3157.64-3159.56:  in credit card debt.
3159.56-3161.12:  Come on, there's no way.
3161.12-3164.88:  Those are just these are sort of outliers.
3164.88-3169.08:  They are points that, well, statistically,
3169.08-3170.6:  some people default, some people
3170.6-3172.4:  don't for reasons of their own, you're
3172.4-3177.4:  going to get points whose labels don't fit the general pattern.
3177.4-3180.3199999999997:  But they're giving you the wrong impression.
3180.3199999999997-3184.6:  If you think that most people who are up here
3184.6-3188.72:  are going to pay their credit card debt on time, you're wrong.
3188.72-3191.88:  There was that one outlying person who paid credit card
3191.88-3193.12:  debt on time.
3193.12-3195.7599999999998:  So when you see boundaries that are this sinewess,
3195.7599999999998-3197.4:  you really should question them.
3197.4-3202.36:  Because is that really fitting the true pattern underlying
3202.36-3204.2400000000002:  what makes people default or not
3204.2400000000002-3206.1600000000003:  default on their credit cards?
3206.1600000000003-3207.44:  Probably not.
3207.44-3210.8:  So sometimes you really need to smooth out
3210.8-3216.2000000000003:  the boundary in a way that looks at the big picture
3216.2000000000003-3219.28:  and doesn't get caught up in all the little details
3219.28-3221.76:  of what this one person did.
3221.76-3224.84:  That is something that linear decision boundaries are very
3224.84-3232.1600000000003:  good at is not getting caught up in too many small details.
3232.1600000000003-3236.32:  So yeah, I like how these three answers got closer
3236.32-3238.52:  and closer to the main point.
3238.52-3239.0:  I guess.
3246.6800000000003-3249.88:  If you have a point on the linear decision boundary,
3249.88-3252.56:  how do you go about classifying that?
3252.56-3256.96:  Usually it's just one arbitrary side is chosen.
3256.96-3259.6:  So maybe I'm going to say all of the points
3259.6-3261.6:  that are on the decision boundary,
3261.6-3264.2:  I'm going to give them a credit card.
3264.2-3268.04:  And we don't really have a statistical basis
3268.04-3269.52:  for making that decision.
3269.52-3275.2:  You just have to pick one and hope it doesn't happen too often.
3275.2-3276.24:  Orange shirt?
3276.24-3279.2:  Do we ever have a case where one model is not
3279.2-3282.9199999999996:  as good as being better than another model that we picked
3282.9199999999996-3286.8799999999997:  up a model that is less computationally accurate?
3286.8799999999997-3287.8799999999997:  Oh, yeah.
3287.8799999999997-3289.3199999999997:  Do we ever pick a worse model?
3289.3199999999997-3292.2:  Because it's less computationally intensive, absolutely.
3292.2-3292.48:  Sure.
3295.7999999999997-3298.04:  You can burn huge amounts of computation
3298.04-3300.0:  on doing really fancy neural network models,
3300.0-3304.64:  but you don't necessarily even have the computer to do that.
3304.64-3306.2:  Sometimes people use super computers
3306.2-3307.08:  to train these things.
3307.08-3308.68:  If you don't have one in your basement,
3308.68-3310.3999999999996:  then you do something cheaper.
3310.3999999999996-3315.04:  And of course, Blue shirt, did you have a question?
3315.04-3316.04:  Yeah, you, sir.
3316.04-3318.52:  I'm sorry, louder, please.
3318.52-3324.04:  Can we make this dynamic?
3324.04-3328.44:  I'm not aware of, well, how do you
3328.44-3329.3999999999996:  want to make a dynamic?
3329.3999999999996-3332.9199999999996:  Like add new training data as time goes by?
3332.9199999999996-3334.8799999999997:  I'm certain that there is research done on that,
3334.8799999999997-3336.8799999999997:  but we won't cover any in this class.
3336.88-3341.1600000000003:  So yeah, people think about that.
3341.1600000000003-3342.08:  Question over there, sir.
3357.1600000000003-3360.84:  So the question is, if we got rid of all those islands,
3360.84-3365.4:  but we still had the really awful bumpy shape for the, say,
3365.4-3368.76:  the main curve, is that also a reason
3368.76-3370.96:  not to trust that decision boundary?
3370.96-3373.12:  Have I paraphrased your question correctly?
3373.12-3374.12:  Yeah.
3374.12-3376.8:  Yes, that is a reason not to trust that decision boundary.
3376.8-3381.36:  The more bumpy and sinew as sin weird it is, the less I trust it.
3381.36-3386.1600000000003:  And so we'll see an intermediate solution in a moment.
3386.1600000000003-3388.88:  I'll take one more question from the back of the room.
3388.88-3390.2400000000002:  If you can talk loudly, please.
3395.4-3398.4:  I'll leave it on the other side.
3398.4-3401.4:  I'll leave it on the other side.
3401.4-3404.7200000000003:  Hmm.
3404.7200000000003-3407.92:  Well, you see, the question is if we had a really large amount
3407.92-3411.6:  of data, how would we know if this island is significant?
3411.6-3415.88:  And my main answer to that is if you get so much data
3415.88-3418.92:  that that island starts filling up with blue points,
3418.92-3422.6:  but it doesn't start filling up with brown crosses,
3422.6-3425.36:  then you start to know it's significant.
3425.48-3427.4:  But as you get more and more data,
3427.4-3430.6800000000003:  maybe you'll find lots of brown crosses in that little island, too.
3430.6800000000003-3435.48:  And then you should forget about it.
3435.48-3436.6400000000003:  All right, I want to move on.
3436.6400000000003-3440.0:  So sorry if there are more questions I missed.
3440.0-3452.6800000000003:  But here is another comparison of two classifiers.
3452.6800000000003-3454.0:  The classifier on the left is going
3454.0-3456.64:  to be exactly the same one we had before.
3456.64-3459.4:  So it's still the nearest neighbor classifier.
3459.4-3462.48:  But what I want to do now is compare the nearest neighbor
3462.48-3467.24:  classifier with what's called a 15 nearest neighbor classifier.
3484.52-3496.96:  So what the one on, by the way, this is still the same data set
3496.96-3498.2:  as before.
3498.2-3500.64:  Two more copies of the same data set.
3500.64-3503.56:  What the classifier on the right does
3503.56-3506.16:  is instead of looking for the nearest neighbor and checking
3506.16-3509.4:  its label, now you look for the 15 nearest neighbors,
3509.4-3514.52:  the 15 training points that are closest to your query point.
3514.52-3515.28:  And they vote.
3515.28-3520.76:  They cast 15 votes, and eight votes gets you a winning class.
3520.76-3523.88:  And so what you'll notice about this is now
3523.88-3527.12:  the decision boundary looks a lot smoother.
3527.12-3529.1600000000003:  That decision boundary looks like something
3529.1600000000003-3532.84:  that maybe you could kind of trust, maybe not every little tiny
3532.84-3534.0:  wobble in detail.
3534.0-3537.96:  But the overall picture looks a lot more like a pattern
3537.96-3543.4:  that might exist in the real world than the one nearest neighbor classifier does.
3543.4-3545.8:  So I'm just more likely to trust that.
3545.8-3546.32:  Yeah?
3552.32-3554.32:  Oh, no difference at all.
3554.32-3560.12:  Just to be totally clear, I am showing you actually the exact same figure twice
3560.12-3561.68:  with no changes whatsoever.
3566.76-3567.52:  All right.
3567.52-3576.64:  So when we look at this, all of these islands, all of these weird zigzags,
3576.64-3580.7599999999998:  we're pretty sure that this is an example of what's called overfitting,
3580.7599999999998-3587.96:  where the decision boundary is hewing too closely to the data
3587.96-3593.12:  and picking up a lot of spurious ideas about the classification
3593.12-3599.64:  that probably do not reflect the true underlying reality of the probability
3599.64-3607.3199999999997:  is involved in how people decide whether to pay their credit card debt.
3607.3199999999997-3612.0:  Whereas the one on the right, just because it's smoother,
3612.0-3615.48:  I think that one's more likely to correspond to a model of reality
3615.48-3617.3599999999997:  that we might have some trust in.
3617.36-3623.28:  All right.
3623.28-3632.6:  So a very brief excursion into what you'll do in homework one.
3632.6-3638.0:  You'll be working with a very famous data set called MNIST,
3638.0-3642.92:  which dates back up three or four decades when people wanted to figure out
3642.92-3645.6:  how to classify handwritten digits.
3645.6-3651.16:  So the data set you're going to get is going to have examples of handwritten digits
3651.16-3653.08:  by many, many different people.
3653.08-3658.4:  And there are 10 classes, the digits 0, 1, 2, 3, 4, 5, 6, 7, 8, 9.
3658.4-3664.88:  So here are some typical sevens and some typical ones taken from this data set.
3664.88-3669.48:  We need to take these images and somehow present them as an input
3669.48-3673.24:  to our machine learning algorithm.
3673.24-3677.12:  Machine learning algorithms are used to receiving points, data points,
3677.12-3681.3599999999997:  which means appointing some high dimensional Euclidean space.
3681.3599999999997-3686.3999999999996:  How are we going to represent an image as a point in a high dimensional Euclidean space?
3686.3999999999996-3688.52:  Well, there's some pretty sophisticated ways you could do that,
3688.52-3691.52:  but we're going to start by doing it in the dumbest way possible.
3691.52-3696.24:  We're going to just express these images as vectors by taking all the pixel
3696.24-3703.4799999999996:  gray scale values and chaining them together in one big array.
3703.4799999999996-3708.9199999999996:  So you could imagine, for instance, that if these images were given to you as pixels
3708.9199999999996-3715.3999999999996:  and each pixel has a value of 0 for black, 3 for white, 1 or 2 for shades of gray,
3715.3999999999996-3723.2799999999997:  then maybe your handwritten digit, well, they're not 4 by 4, they're really 28 by 28,
3723.2799999999997-3725.4799999999996:  but pretend they're 4 by 4.
3725.52-3732.88:  Maybe it looks like this, and so what you do is you just write this out
3732.88-3737.08:  as one big vector of length 16.
3741.04-3750.92:  And we're going to think of this as a point in 16 dimensional space, 16 dimensional Euclidean space.
3750.92-3755.08:  And so a lot, there's a lot of geometry in machine learning,
3755.08-3760.0:  and so you're going to get need to get used to thinking of the data that you work with
3760.0-3763.96:  as being points in some high dimensional space.
3769.24-3773.88:  Now, when you have more than two dimensions, more than two variables,
3773.88-3778.44:  more than two pieces of information about each credit card holder,
3778.44-3780.96:  we can still have linear decision boundaries,
3780.96-3783.24:  but the decision boundary isn't a line anymore.
3783.24-3787.7999999999997:  It's a high dimensional, 15 dimensional thing called a hyperplane,
3787.7999999999997-3790.6:  and I will teach you all about hyperplane's next lecture.
3796.68-3800.8799999999997:  But this is what you need to know right now to do homework one, so there you go.
3800.8799999999997-3804.12:  What I want to spend the rest of the time in is something very important,
3804.12-3806.08:  also important for homework one.
3806.08-3814.7999999999997:  The main thing that I think we do right as machine learning people is that we make sure
3814.7999999999997-3820.44:  that our models that we train have statistical validity.
3822.56-3827.16:  And so I want to talk about the difference between training error, test error,
3827.16-3831.2:  and validation error, and also what is validation,
3831.2-3834.08:  and why is it the most super important part of machine learning.
3836.08-3839.48:  Here's how we do classification.
3839.48-3842.96:  First of all, we assume that we are given a bunch of labeled data.
3848.36-3856.72:  And labeled data means we're given sample points, points in say two dimensional space
3856.72-3859.84:  or 16 dimensional space or a million dimensional space.
3859.84-3865.6000000000004:  And each of those sample points comes with a class label.
3865.6000000000004-3870.0:  And as I said before, often we have two classes, but sometimes you have more.
3870.0-3875.04:  Maybe you want to distinguish whether your photo is a dog, a cat, or a human.
3875.04-3876.6000000000004:  That's a three class problem.
3879.6000000000004-3882.96:  So here's a really important part of what we do next.
3882.96-3886.88:  We're going to hold back a subset of this labeled data.
3886.96-3894.32:  We are not going to use all of the labeled data to train our classifier.
3894.32-3895.84:  We're going to hold some of them back.
3895.84-3899.76:  And the ones we hold back are called the validation set.
3904.04-3907.2000000000003:  And these are what we're going to use to make sure that our model
3907.2000000000003-3911.84:  that we train at the end has statistical validity of some sort.
3912.8-3917.1200000000003:  So maybe 20% of the points is typical, is what's often done.
3918.1600000000003-3921.2000000000003:  We'll be held back as the validation data.
3921.2000000000003-3926.56:  The other 80% of our labeled data is called the training set.
3926.56-3932.8:  And the training set is what we actually use to allow our classifier to learn something.
3934.6400000000003-3936.32:  I want to give you a big warning though.
3936.32-3939.6000000000004:  When you actually are out in the world of machine learning,
3939.6-3944.24:  you will find that people do not use the term training data in a consistent way.
3945.12-3949.04:  Often training data refers to all of the labeled data.
3949.04-3952.0:  In fact, even in homework one, which we're handed out,
3953.36-3957.2799999999997:  we say training data when we really mean all the labeled data.
3957.2799999999997-3959.36:  You're not going to use all of it to train.
3959.36-3961.8399999999997:  You're going to hold some of it back as a validation set.
3963.2799999999997-3965.36:  And so when someone says training data,
3965.36-3968.7999999999997:  you have to judge from context whether that's all the labeled data
3968.8-3970.0800000000004:  or just the training set.
3972.96-3975.92:  All right, now that we've held back 20% of our data,
3975.92-3984.1600000000003:  we are going to train one or more classifiers on the other 80% of our data on the training set.
3986.5600000000004-3990.1600000000003:  And so those classifiers are now doing what we call learning.
3990.1600000000003-3994.7200000000003:  They learn how to distinguish seven from not a seven, for instance,
3994.7200000000003-3996.7200000000003:  if you're doing the digit recognition problem.
3999.76-4007.1200000000003:  And you'd have a different classifier as to distinguish three from not three and so on.
4008.6400000000003-4011.2000000000003:  All right, and so we are using the training set
4012.8-4016.96:  to learn the weights in the model.
4018.48-4023.52:  Well, not all models have weights, but most of them have little numbers called weights
4023.52-4027.04:  that we need to figure out that we need to optimize and learn.
4029.44-4031.1200000000003:  That'll be a mess in for another day.
4031.1200000000003-4044.1600000000003:  The lesson for today is you do not use your validation data to train.
4055.44-4057.36:  All right, now usually what we're going to do,
4058.88-4062.96:  is either we're going to train multiple different learning algorithms,
4070.6400000000003-4077.36:  or more commonly, we're going to train multiple instances of one learning algorithm.
4079.44-4083.28:  But with multiple different hyperparameter settings,
4084.2400000000002-4087.36:  I'll say more about what a hyperparameter is in a moment, but
4089.1200000000003-4100.16:  learning algorithms often have knobs that you can turn to get different results and different classifiers.
4100.16-4101.360000000001:  Or you might do both.
4101.360000000001-4106.0:  You might train multiple different learning algorithms in each of them with multiple different
4106.0-4107.28:  hyperparameter settings.
4108.320000000001-4114.4800000000005:  But the important thing is that you're using the same training set for each one of these different
4114.4800000000005-4115.92:  classifiers that you train.
4116.4-4121.84:  All right, so now you've got a bunch of classifiers and you want to know which one is the best one,
4121.84-4123.2:  and that's where we validate.
4123.84-4128.8:  You validate the trained classifiers on the validation set.
4134.32-4137.68:  And here's where we learned if our learning was actually worth anything.
4137.76-4146.320000000001:  So essentially what this means is that you're going to take all your classifiers,
4146.320000000001-4151.68:  you're going to ask them to make predictions on the validation set.
4152.320000000001-4155.6:  You don't tell your classifier what the correct labels are,
4155.6-4159.84:  you ask your classifier, what are the correct labels, and you compare with the labels that you
4159.84-4163.200000000001:  actually have, and now you know if that classifier is any good or not.
4164.16-4173.44:  So then we're going to choose the classifier or the hyperparameter settings or both that
4173.44-4175.36:  made the most correct predictions.
4178.08-4183.2:  We're going to put it another way that had the lowest error on the validation set.
4185.599999999999-4193.12:  I will define validation error shortly, but this whole process of testing your classifier
4193.44-4198.4:  is to see if they're really good and picking the best one is called validation.
4203.84-4210.4:  Now again, I want to emphasize when we do validation, our classifiers are not learning anymore.
4211.04-4217.76:  All they're doing is they're taking in points that are in our validation set and making predictions
4217.76-4222.24:  on those points. We then check if they made the right predictions or the wrong predictions.
4222.8-4225.679999999999:  We assign each classifier a score. We picked the best one.
4228.96-4231.679999999999:  And so we're using this to judge our models.
4232.32-4237.04:  What we do not use to judge our models is how well they learned the training set.
4239.04-4243.12:  I mean a lookup table can learn the training set with 100% accuracy. That's not impressive.
4246.32-4249.04:  We need to test if these things can guess for new points.
4249.04-4262.32:  Now there's an additional step that's often called testing, but the set that is used to do it is called a test set.
4265.28-4272.96:  The test set again involves new data that was not in the original set of label data that you were given.
4273.44-4276.64:  You can think of this as being the final evaluation of your model.
4279.68-4286.64:  And the important thing here is that typically you do not have the labels.
4297.5199999999995-4302.96:  Some bought somebody else might have the labels and that somebody else might assign you a score.
4302.96-4306.08:  You trained a good classifier. You trained an awful classifier.
4306.88-4310.88:  Sometimes you do have the labels, but you hide them from yourself.
4310.88-4318.0:  You pretend you don't have the labels. You do the run your test and then you take the labels out of the vault and see how you did.
4318.0-4324.88:  Or maybe the labels are kept by a third party who verifies them for you.
4324.88-4328.16:  And that's what we're going to do in the homeworks with this website called Kaggle.
4328.5599999999995-4334.96:  By the way, just a quick aside, you've noticed I've been underlying words.
4334.96-4338.8:  Underlining words here. When I underline a word, that's a definition.
4338.8-4350.16:  You should memorize those definitions. That will help you do well in this class because you were expected to know the precise, rigorous meaning of those underlined words.
4350.16-4354.16:  They're not just random words. They're ones with strict definitions.
4358.88-4361.68:  All right, so I define three different kinds of sets.
4361.68-4370.16:  And accordingly, there are three different kinds of error called training error, validation error, and test error.
4371.12-4378.16:  So the training error is the fraction of the training set that is not classified correctly.
4388.24-4401.36:  As I remarked before, the training error is zero with the one nearest neighbor classifier, which makes the one nearest neighbor classifier sound very impressive if you're naive, but it's not impressive.
4401.36-4406.96:  I look like I said a lookup table can do that, but a lookup table can't generalize to new points.
4407.04-4414.4800000000005:  The training error is not zero with the 15 nearest neighbor classifier.
4414.4800000000005-4419.76:  It's not zero with the linear classifier, but often those are better classifiers predicting new data.
4421.36-4430.08:  And so one thing I want to say about training data is again, remember, the validation data does not go into this calculation.
4430.16-4446.08:  Even if somebody calls it training data, the validation set, we keep it out separately and we compute its own error and the error just on the validation data.
4446.08-4453.6:  So the validation error is the fraction of the validation set that is not classified correctly.
4453.6-4465.04:  And this is what we're going to use to choose our classifier.
4467.04-4473.120000000001:  And if our classifier has hyper parameters that we have to set, we're going to use it to set our hyper parameters.
4473.12-4487.44:  Now, remember that you did not use the validation data to train your classifiers unless you're bad at this.
4487.44-4493.12:  So even the one nearest neighbor classifier can classify the validation set wrong.
4493.599999999999-4505.2:  The validation error is almost always higher than the training error with almost any classification algorithm.
4505.2-4507.2:  That's okay, you should expect that.
4507.2-4510.48:  If the validation is error is lower, you should always be suspicious.
4510.5599999999995-4523.44:  Well, then there's the test error, which is the fraction of the test set, not classified correctly.
4526.799999999999-4530.24:  And the test error is used to evaluate you.
4536.16-4538.24:  Do you know how to construct a good classifier?
4540.5599999999995-4548.0:  All right, let me come back to hyper parameters.
4548.0-4559.2:  Most ML algorithms have at least one or two parameters that you can adjust to get different results,
4559.2-4563.679999999999:  especially maybe smoother or less smooth decision boundaries.
4564.56-4576.16:  So often, but not always, the things that we call hyper parameters control a trade-off between overfitting and underfitting.
4579.280000000001-4586.320000000001:  And one example of a hyper parameter is the parameter k in the k nearest neighbor classifier.
4586.719999999999-4600.0:  So let me show you a chart, a graph, that comes out of one of your textbooks from
4600.0-4608.24:  Elements of Statistical Learning. And it's pretty useful for showing what happens with nearest
4608.24-4612.88:  neighbors. All right, so this chart's a little bit weird, a little hard to understand because
4612.88-4618.0:  there's so much going on to take in first. The first thing I want you to do is look at the x-axis,
4618.72-4628.88:  which is k. So k is, I'm going to take a query point that I want to classify. I look for the k points
4629.52-4637.92:  in the training set that are closest to this query point. And they've done something very weird here,
4637.92-4644.4800000000005:  they have k going up from right to left. So here's what happens when you have one nearest neighbor.
4644.4800000000005-4649.28:  Here's what happens when you have three nearest neighbors. Here's what happens when you have five nearest
4649.28-4655.12:  neighbors. Why did they do this? Well, we often like to plot these graphs in machine learning so that
4655.12-4660.24:  underfitting is on the left hand side and overfitting is on the right hand side. So that's what they did.
4660.8-4666.08:  And I know it's a little confusing, but understand that k is one at the right and then goes up to
4666.32-4678.4:  151 nearest neighbors at the left. So that's the x-axis. What is the y-axis now? Well, there are two
4678.4-4685.92:  lines graphed here. You can see that there's a blue line, which is the training error. You can see
4685.92-4692.0:  that there's an orange line. The orange line, you can think of it as either the validation error
4692.0-4696.08:  or the test error. It doesn't really matter. They both behave the same way usually.
4698.96-4707.36:  And what you'll notice about the training error is that, well, as we,
4710.32-4717.52:  when you have one nearest neighbor, the training error is zero, as I said before, one nearest
4717.52-4721.6:  neighbor always classifies all the training points correctly. So the training error is zero,
4722.320000000001-4729.280000000001:  which looks great, but no, no, no, no. As you increase k, the training error gets worse and worse.
4729.92-4736.160000000001:  When you get to the extreme of 151, well, you're just making random choices. That's not good.
4739.76-4747.200000000001:  But what's interesting here is that the test error tells a different story than the training
4747.679999999999-4756.0:  error. What the test error says is that we have a ushaped curve that the best choice is not the far
4756.0-4761.92:  left or the far right. The best choice is seven nearest neighbors right here. That's what gives us
4761.92-4770.48:  the lowest test error. If we go to one nearest neighbor, well, look, we're overfitting. Our test error
4770.48-4775.76:  actually starts getting worse as k goes below seven, because we're getting these sinuous
4775.76-4781.2:  boundaries that don't mean anything that aren't correct. But if we go to the other extreme,
4781.2-4788.96:  if we make k too large, then now we're underfitting. We are smoothing out the boundaries so much
4789.52-4795.6:  that the decision boundary no longer means anything. So for many machine learning algorithms,
4795.6-4800.64:  we will find that there's a happy medium between overfitting and underfitting, and that's what we're
4800.64-4806.320000000001:  looking for. By the way, neural networks are a famous exception to this rule. But most of the
4806.320000000001-4811.4400000000005:  algorithms we study this semester will have this property that we're on a tight rope between
4811.4400000000005-4825.68:  underfitting and overfitting. So let me just define those terms. What do I mean when I say overfitting
4825.68-4834.8:  or underfitting? Well, these aren't totally rigorous definitions, but they're phenomena that we
4834.8-4841.04:  often see in practice, and we will have some statistical ways of understanding these a little bit
4841.04-4849.6:  later in the semester around lecture 12. But as concepts, these are great. These are really good
4849.6-4855.52:  things to understand. To see why your data acts the way it does. Why your classifiers act the way
4855.52-4862.56:  they do. Overfitting is when the validation error or the test error deteriorates because the
4862.56-4871.68:  classifiers just becoming too sensitive to spurious patterns or outliers. I'm going to define outliers
4871.68-4890.08:  in a minute too. Underfitting, well, it's at the other end of the graph, so it's sort of the
4890.08-4901.76:  opposite, but it's not really an opposite. It is when the validation and test error also deteriorates
4902.4-4911.04:  for a completely different reason, because now your classifier is just not flexible to fit the
4911.04-4934.64:  patterns that you are trying to fit. And one of the main causes of overfitting is outliers. Outliers
4934.64-4941.52:  are points with atypical labels. Now, the extreme version of an atypical label is a data entry error
4941.52-4947.76:  where somebody just entered the wrong class into the computer, so you've got a point totally in the
4947.76-4954.0:  wrong place. But you have to actually worry about statistical outliers too, which may be real,
4954.0-4960.56:  correct data, but just representing the vagaries of human nature. So maybe a borrower is very,
4960.56-4971.68:  very rich, but defaulted anyway. Why we don't know. And the important point about outliers is that
4971.68-4977.76:  the more you have, the more your risk of overfitting, and the more you need a method that will smooth out
4977.76-4983.200000000001:  your decision boundaries a little bit. Because in machine learning, the goal is to create a classifier
4983.200000000001-4989.6:  that generalizes to new data that we haven't seen yet. And so overfitting and underfitting are both
4989.6-4995.04:  counterproductive to that goal in different ways. We're always seeking a compromise. We want decision
4995.04-5003.04:  boundaries that make fine distinctions, but aren't downright superstitious. Last thing I want to mention
5003.04-5009.52:  is that for homework one, the way that we will be running your test set is on a website called Kaggle.com,
5010.160000000001-5015.04:  which is designed to run machine learning competitions, including our homeworks.
5015.36-5027.2:  And so you will have no access to the labels of the test set, but Kaggle will and Kaggle will assign you a score.
5028.4-5034.24:  Thanks very much and on next Monday we'll talk about decision boundaries and linear classifiers some more.
5034.24-5036.24:  Thank you.
