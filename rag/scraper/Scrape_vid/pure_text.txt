 .. Hello everyone, welcome to CS189, or welcome to CS289A, if you're a grad student, machine learning. My name is Jonathan Schuchak, I'll be your professor this semester. The things that you need to know to, or the things you need to have access to to be part of this class, even if you're not registered yet, are the class website, which is up there, it's public, anyone can read it. You'll need access to the Ed discussion group where we discuss the course, and there is an invitation link, link from the class webpage, you can use that to get to our Ed discussion group immediately. The other thing you'll need access to is our grade scope, which is where you submit your homeworks. The, you will need, if you're not already registered in grade scope as some of you are, you'll need to get the access code to get into grade scope. The access code will be published sometime in the next 24 hours on the Ed discussion group. So your procedure is, go visit the class webpage, read it if you have time, because it's worth knowing what you need to do this semester. Join our Ed discussion group, check the Ed discussion group sometime tomorrow, and look for the enrollment code for grade scope, if you haven't already received an email from grade scope saying you're in. And even if you're not enrolled in the course, this will work for you. Let me be clear that you do not need access to B courses. If you send me an email asking for access to B courses, the answer is no, I don't have time. It's not important that you don't need a B course access. Once you're enrolled, you will get B course access, which might matter later in the semester, but not now. The class webpage, one reason to visit it is because I have all the readings for the semester written down. So I am not going to remind you during lectures of what your readings are. It's your responsibility, as you know, an adult attending university, too. Go to the class webpage, find out what readings are expected of you for each week, and actually read them. There will also be, I'll make my lecture notes available after classes, but I want to be clear that the primary purpose of the lecture notes is so that I know what to say during lecture, so you don't expect it to read like a textbook. But you might find it useful. Homework One is already available from the class webpage. You can go right now and download it and get started. It's due one week from now. Part of the purpose of Homework One is to make sure that you're ready to do the rest of the homeworks, that you're familiar with the things that you need to do with loading files, writing files, doing basic machine learning computations on files, submitting your answers to Kaggle.com, which I'll talk about later. The bad news about Homework One is that we're going to ask you to do something that uses what's called a support vector machine to do classification, but we don't actually have time to teach you support vector machines until after the homework is due. But the good news is you don't actually have to implement support vector machine. You're just going to use a built-in support vector library and call it. And so the homework is really focused not around you having to know everything about support vector machines. It's more about learning the basic infrastructure of things you need to know to do the rest of the homeworks coming up later this semester. So please get started on that early. One of the things that you need to do as part of this homework is Kaggle submissions. You will be limited to two Kaggle submissions per day. So if you start the homework the morning it's due, you only get two Kaggle submissions, which probably isn't enough. So start early on the coding part of the homework, please. You'll have lots of questions no doubt this semester. And other, of course, you're welcome to come to our office hours and discussion sections, but also our primary platform for electronic communication is going to be at discussion. And I want to ask that almost all communication of that form, like online should go through at discussion. Please resist the urge to send too many emails, because I want to be clear that I am going to be checking at discussion far, far more often during the semester than I'll be checking email. When I teach a course I really follow the discussion group closely, and my email gets even worse neglected than usual. However, sometimes you might have a private matter to discuss. Sometimes something personal is going on in your life. You need to talk about, or sometimes you might want to share a large piece of your code for your homework in hopes that we can help you. And it's not appropriate to share large pieces of your code with the rest of the class. In those cases, Ed discussion allows you to make private posts. And the private posts will be viewable only by me and the TAs. And so feel free to use private posts when it's appropriate for one of those two reasons, personal issues, or your posting solutions, or a lot of code, or something that you're not allowed to show to your fellow students. But we want to ask you to make most of your Ed discussion posts public, because the questions you ask, probably there's 100 other people in the room with the same question. And if we can answer 50 of them at once, it benefits everybody in the class. Now, if you have something that's really, really personal, like you don't even want the TAs to know about it, you only want me to know about it. Then, of course, it's appropriate to send a request to my email address, that one there. But again, I'm kind of bad about checking that, compared to checking Ed discussion. Just so you know. And this class will have discussion sections, which are optional. I should mention that the lecture itself, personal attendance is optional. Nobody's taking attendance. That every lecture is going to be videotaped. You can watch the videos at home if you want. So attendance in most things is optional, though you do have to attend the exams. Having said that, I think it's really good for you to attend a discussion section in person each week. The sections are wide open. Attend any section you want. If you want to attend every section, you can attend every section. We don't care. The sections, as you probably already noticed, they were done this week. They start next week. They'll be on Tuesdays and Wednesdays, starting next Tuesday. A lot of you are probably really wondering about enrollment. We currently, as far as I know, we're going to have room from maximum of 703 students in this class. Right now, when I last checked this afternoon, there were 452 wait listed students. So we are expecting many people to drop the class. And I'll say more about why later. But this is a class that we realize heavily on continuous mathematics. And homework, too, is kind of going to be your personal self-test of whether you're ready for this class. And if you don't have the prerequisites, that's no shame. You might want to consider putting off the class to another semester and getting more of the prerequisites first. Hopefully, by the time you finish homework, too, you'll know. And I should add, the homework, too, deadline date, is the same as the drop deadline for the class. And that's not a coincidence. It's there because we want you to be informed about whether you're making the right decision to drop or to stay in the class. Typically, what we've experienced in the past is maybe there are 100 plus drops, or sometimes 200. So if you're on the wait list, there's still a good chance of getting in. I've been teaching this class annually, most years since 2016. And every semester, somehow magically, we managed to clear all the wait lists by the time of the drop date. But there were hundreds of nervous people right up to the drop date. So that's just how it goes. I can't promise that we'll be able to clear the whole wait list this year. I hope so, but I don't know. We'll see. The prioritization kind of works like this. So X grads have the highest priority. If you were a graduate student in X, we won't find a way to get you into this class. So if somehow you're not in the class, let me know. The second after that, under grads, the class as far as I know is being limited to CS and data science students. They're the only ones who can even get on the wait list. I expect most of those will probably clear. Third in line are grad students who are not in X. And I'm sorry about that, but we have to take care of our own first. I hope we'll be able to accommodate a lot of non-EX grad students. But at this point, I just can't tell you. We will be able to admit a small number of concurrent enrollment students, partly because concurrent enrollment students give us extra money we can use to take in more of our own Berkeley students. But not a lot. And I think there's 100 plus concurrent enrollment applications. We won't be able to take half of those even, but we'll take some. We'll see what happens. And if you are hoping to get enrolled in the class, but you're not enrolled yet, you should participate normally. You should submit your homeworks. Don't, if you finally get admitted to the class three weeks from now, and you haven't been doing your homeworks, that's on you. We're not going to give you an extension. So you have all the tools. You will have all the tools you need through a discussion and grade scope to participate in the class, even if you're not enrolled yet. So don't worry about that. All right, that's the high level logistics. Do we have questions about that? Yes, sir? So wait, what's the same for undergrad students? The wait list is not the same for undergrad students. And we are manually prioritizing them, as I said, ex-grads first, undergrad second, non-ex-grads third. So also, whatever it says is the total enrollments in 189 or 289-and or classes.berkeley. You can't take that seriously because 703 is the maximum number of students we can enroll in both 189 and 289-a combined. And we have to manually trade off those two. The system doesn't do it automatically. So you cannot take seriously whatever it says about how many seats are available. Other questions? I have some good news about textbooks. If you're satisfied with online versions or PDF versions, your textbooks will be free of charge. Having said that, I think that the two textbooks that form the main reading for this course are well-written textbooks and that the author is deserving your money. So if you're inclined to buy a paper copy, then either these textbooks are both. I think that that's a nice thing to do to the authors, who I think have done a lot for us. But if you're short on funds, I totally understand. Just download the PDF files and use them for your readings. No problem. And so here are our textbooks in introduction to statistical learning, elements of statistical learning. They have some authors in common, and they have some material in common. But the class web page will tell you which chapters of these books or sections I think you should read. And I like these books. But you're going to see that some things are done a little bit differently in this class, especially these books avoid matrix notation. And we're definitely not avoiding matrix notation, because we want to be able to understand things a little more deeply in this class. We also, of course, this being a computer science class, I'm going to talk more about how some of these things are actually computed, how we actually compute solutions to a lot of these machine learning problems. And whereas these books kind of are less about that, more about the statistical part and the application part. But they're very good for the statistical part and the application part. Back to the prerequisites for this class. I want to say in advance that we're not formally enforcing any prerequisites. And part of the reason for that is because it's more important what you know and not important, what particular class or reading you learned it from. But we're going to use a lot of tough continuous math. And you need to be ready. And whether you're ready is maybe not a function of whether you've taken a class, or although that can help a lot. But whether you actually really understood the class. So the first thing you'll need to know is vector calculus. And if you've taken math 53, you're probably in good shape, though that's not the only way that you could get this prerequisite. The amount of vector calculus you need to know is actually pretty limited. You need to understand gradients, and you need to understand them really well. You need to understand the multivariate chain rule really well. Those are the two things we need. If you don't remember div and curl, we're not going to use those. You don't need those. You need gradients. You need the chain rule when there's more than one intermediate variable. There are two bigger chunks of knowledge, though, that you really need to have a good handle on that are. One is linear algebra. And you need to know a lot more linear algebra than you need to know about gradients. There are many, many different ways you might have gotten that information here at Berkeley. Here's three possibilities. There are other linear algebra courses here and at other universities. So there's many different ways you could prepare for this. And also, I want to say, though, that to some degree, what's going to matter more for this course is not can you do the exercises in the algebra in a linear algebra course? But did you really understand it? Did you have a geometric understanding of what they taught you in your linear algebra class? And I find not all classes are good for that. When I was undergrad, the linear algebra class I took was absolutely horrible. I learned all kinds of algebra and had zero intuition for any of it, zero geometry. And that did not put me in a good shape. But eventually, I recovered. One thing that I really strongly recommend, and this is linked from the class web page, on YouTube, there's someone who goes by the name, I think three blue, one brown, or something like that. It's linked from the class web page anyway. So if I got it wrong, you can find it there. Has a really great series of videos on the fundamentals of linear algebra that show you the visualization, show you the geometry. So you really know what all that algebra actually means. And having that is going to be a huge advantage in understanding what we do in this course. So please watch that if you haven't watched it before. That would be a great place to start prepping right now. Because watch those three blue, one brown videos. I wish I knew a resource as good for learning probability because you need to have a really good understanding of probability as well. Here are some places where you might have learned enough probability for this class. And for all three of these things, there are resources linked from the class web page. If you want to brush up and try to swap in all the things you've forgotten about these three bits of math. The last thing that you need to function well in this course is enough programming experience. And the thing to understand here is that although this is a computer science course, it's an advanced computer science course. And we assume that everybody coming in knows how to deal with large programming projects, knows how to debug your own code, knows data structures, like take and see 61b you're an equivalent. And the most important thing I want to make clear is that the teaching assistants in this class have no obligation to look at your code. If you have a bug and you can't solve it, well, if a teaching assistant is nice enough to help you debug your code, that's a bonus. You should be extra, extra thankful because that's a really nice teaching assistant. But they're going to be told that they're under no obligation to help you debug your code. This is not an introductory programming class. So please make sure that you are good at debugging code if you're going to take this class. One thing I just want to clarify is not a prerequisite at all is CS188. If you haven't taken the introductory artificial intelligence class, don't worry. Not a problem. Just quickly, how are we going to grade this course? 40% of your grade is going to come from seven homeworks. And these are at least initially all planned to be due on Wednesday nights at 11.59 p.m. And in order to help you navigate major problems, we're going to offer you five slip days total cumulative overall the homeworks. So if you submit homework to two days late and homework three, three days late, then no penalty. Once you go over five slip days, your homeworks are just not accepted. So five days is a pretty strong limit. We count by the day. We do not count hours. So if you're one minute late, that counts as one day of slip days used. And if you are in the disabled students program, probably you're going to have additional extensions available. And yes, extensions and slip days can be combined if you're in that situation. But one thing I want to make clear is that we do not accept any homework more than five days after it's due, even with a combination of extensions and slip days, because we have to grade them sometime. 20% of your grade will come from the midterm. And I'm sorry to tell you that I don't yet know what day the midterm will be on. But very likely it will be either the Monday or the Wednesday before spring break. But we're still waiting to find out when we can actually get rooms so that we can have alternate seating for the midterm. The time of the midterm will be sometimes during 6.30 to 8.30 PM. Probably, though, this is not a promise. We might have to change that at the last minute. So please don't consider that a promise. But the actual midterm will be 80 minutes long. So you don't actually get two hours. You get 80 minutes. But if we may, because of the room situation, we might have some of you starting the exam at 6.30 and some of you starting at 7 in a different room. I'll let you know on that discussion as soon as we know when the midterm is going to be. So sorry, we don't have that sorted out yet. But getting rooms from the university is complicated sometimes. We do at least know the time, though not the room of the final exam. The campus has announced that it will be on Friday, May 10th, from 3 to 6 PM. And I think you will have the full three hours to do that exam. If you're a grad student, and therefore you should be in 289A, it's almost the same except that you also have to do a project. And so you're going to have the same amount of your score coming from the homework in the midterm. But the final exam will be worth only 20% of your final grade. And the other 20% will be the project. These are a team project. You're expected to work in a team of two or three students. You're expected to do a project that's related to this class somehow, but we're pretty flexible. And we hope that you will find a project that helps the research of at least one of you on your team or is of great personal interest to at least one of you on your team. And so we'll talk about that more later in the semester. Sadly, I need to talk about cheating. It's not allowed. But what is allowed is discussion of homework problems. So you can talk about homework problems with each other. Just don't write down your answers for each other. Now, regarding sharing code, I think it's OK to show other students small snippets of code if it will help them if it's with simple things. And indeed, you are allowed to share small snippets of code on a discussion as well if it doesn't give away too much of the answer in your judgment. What do I mean by small? Well, probably 10 lines or less. Unless those 10 lines are the complete solution to a problem in which case maybe that's too much. But don't share large amounts of code. If you're going over 10 lines of code, that's a red flag. Now, all homeworks, including, of course, all the programs in your homeworks, have to be written individually at the end. I mean, you can discuss math problems with each other. But in the end, you have to write up your own solution. And we're going to actively check code for plagiarism with automatic plagiarism checking software. And if you get hot cheating, we don't just give you a zero. The typical penalty is a large negative score. Like typically negative, the equivalent of getting negative 100% on that particular homework. And so it's going to bring down your other homework scores as well. Having said that, this is not a limitation on what I can do. I reserve the right to give an F to anyone caught cheating once, if I think it's egregious. At my discretion. And if you're caught cheating twice, then that will always be an F, of course. I haven't had big problems in 189. But the last time I taught CS61B, we had to punish roughly 100 students for cheating. And it was a very painful experience. And so please don't put me through that again. All right, that's the end of the administrative part of this lecture. Are there any questions? Yeah. Sorry, for the what portion? Probability. So certainly needs to know all the basic probability about understanding coin flips and things like that, binomial distributions. We're going to use a lot of continuous distributions, especially the normal distribution. The normal distribution, I think, is badly enough hot that I need to teach it again. So there will be some lecturing on that in this class. But definitely understand as much of the univariate in normal distribution coming in as you can. Understand means and variances very well of continuous distributions. That's the most important part. Question there. The homework format, we do allow handwritten homeworks if your handwriting is neat. We like to encourage students to do late hack if they can. Late hack is one of the crown jewels of computer science and if you haven't learned how to use it, it's a great opportunity to take the time to learn how to use it because that's a fantastic skill to have. But yes, we do accept handwritten homeworks as long as we can read them. With a proviso that with a handwritten homework, if we can't read your handwriting, you don't get to an appeal on that. It's that's purely our judgment. Yes. The internet for programming assignments. You should not be copying code that's actually meant to solve the same problem you're solving essentially. I know that people will search for a general clues on just how to program in Python, how to use certain libraries. Of course, you have to do that to get through this class. So that's going to be normal and expected. Just, but you need to write your own algorithms at the end of the day. Other questions up front? I will quickly teach multivariate Gaussian again because as I said, I think it's usually not taught well enough. But I really, if you've never seen it before, that's not a good start for this course. So I hope you've seen multivariate Gaussian's taught at least badly once before you come into this course. I definitely hope that you understand univariate Gaussian's really well. That's what I would say. Other questions? Ah, yeah, over there. I think it's a suggestion for the problem or like, do you have any clues or questions? Oh, no, no. You do not need to know anything about R at all. You probably want to know Python for this class. Although we do not have a requirement for what programming language you use, your life's going to be a lot easier if you're using Python because everybody else in the class is. I should mention, I forgot to say, this is a new thing. Just this summer, a new version of this textbook came out with a fifth author who rewrote all the R applications in Python. So there is a version of introduction to statistical learning with applications in Python. And you might want to download that version instead. It's also free on the internet at the same link as this book. But the readings, the reading assignments, I give you, the chapter numbers and the section numbers are going to be for the R version, sorry. So you'll have to cross-map from the R version to the Python versions, know what the correct readings are. All right. Administrative part over, let's start talking about what this class is about. If I were asked to boil down this class into three phrases, here they are. We are going to think about finding patterns in data. And unlike a statistics class, we're not just doing it for the sake of doing it. We're going to use them to make predictions. How do we find patterns? Well, we're going to have models and statistics that help us recognize and understand those patterns. And we're going to learn about optimization algorithms that actually do the learning of those patterns. So there you go, machine learning in three pithy phrases. The most important word in those three pithy phrases is the data. Data drives everything else. If you do not have enough data, you really can't learn anything. If you're data sucks, you can't really learn anything. And it's very common to have data that's really not good enough for the machine learning task you want to do. But it is amazing what you can do when you have lots of good data. So for instance, one thing that you can do now that people have done is you can download millions of photographs from the internet, people's travel photos, and use them to create a 3D reconstruction of Paris. That's awesome that you can do that. And this has been a big change in machine learning. Because machine learning people up to maybe 1995, 2000 didn't have access to that kind of data. And then suddenly they did. And this has changed machine learning in a lot of ways, techniques that were formerly out of fashion, like neural networks, came back into fashion specifically. Because when you have huge amounts of data, neural networks shine. They didn't went back when we could only work on small data problems. Well, partly we reprogramming neural networks wrong too, but that's another story. So the neural network has given us access to huge vast quantities of data of all different kinds. And that's just changed machine learning practice a huge amount. The most common task that you'll hear about people doing with machine learning is called classification. Very famous examples of classification are your computer looks at a photograph and tells you, oh, there's a cat in that photo. How do we do that? Well, let's go back and think about what classification is. It'll take us the whole semester to answer the question, how do we recognize the cat? But let's get started. All right, so here's a much simpler than recognizing a cat classification problem. Here you are a credit card company. And you have data about a bunch of your credit card holders and which ones default on their credit card payments and which ones don't. And so in this figure here, we have a bunch of blue circles who are the people who are paying their credit cards on time. And we have a bunch of brown crosses who are the people who defaulted on their credit card payments. And we have some information about them. For each of these people, we know their annual income and we know what their average credit card balance is over all their credit cards each month. You can get this information through credit services and other credit card companies. And you can also get this information about people who are applying for credit cards. So someone says, hey, give me a credit card and you say, should I give this person a credit card? Well, you can ask for their income. You can get the information about their other balances at other banks and compare it with the people you already know and make a decision about what you think is the probability that they're gonna default on their credit card debt. So how would you do that? So let's say this green dot here, that's a new applicant. Says, hey, I want a credit card. Our job in machine learning people, as machine learning people, is to make that decision. So let me just formalize what we're doing here. First of all, we've collected a bunch of data and that data is typically called training points. Why points? Well, because those are points, we have different information about each person. Their income, their average monthly credit card balance. And we think of those as just coordinates in some Euclidean space. And so each person becomes a point. Well, each person becomes a point and a label. The label is good person who pays on time or defaulter. So the training points are points because they have income and balance information, which gives them a position in two-dimensional space. And they also come with an additional piece of information which we call labels. And the label tells you for each point what class it is in, either reliable debtors. Or defaulted debtors. Those are the two classes in this particular problem. And just to be clear, both the words labels and class, those are technical terms here. The label is the actual information you're given with each person about whether they defaulted or not. The class is one of these two choices in this particular case. Sometimes there are more than two choices, but we often work with two class problems like this. And then we want to do prediction. The whole point of machine learning is usually to do prediction. So in this case, we want to evaluate new applicants. In other words, we want to predict what class they're going to be in. They don't have a class yet, or maybe they do, but we don't know. So we try to guess their class. So I've got this green point up here. And I have to make a decision. I can say, I think this person is not going to pay their debt. And I say, sorry, we're awful. Or you say, I think this person is going to pay the debt. So you say, here is your new credit card, ma'am. Don't tell your husband. So when you're doing that, we could think of several ways you might do this here. And the first thing you might notice when you look at the existing debtors is that the ones who have high balances seem to default a lot. And the ones who have low balances don't seem to default a lot. So maybe you could say, OK, it looks like maybe 1,300 is the right cutoff. Let's say that everybody with a balance over 1,300 will probably default. And everyone, under 1,300 is likely to pay. And that seems like it predicts a lot of the existing people, a lot of the time. So in this case, you'll say, OK, I think that this green point here is on the left side of that line. So let's give that person a credit card. Or I could say, well, I can be more sophisticated than that, because it does look like income has an influence as well. A small influence, but nevertheless, I think that I can maybe tilt my line a little bit and get a slightly more accurate division between people who default and people who don't default, because people with higher incomes, they seem slightly less likely to default. But if I can do that, maybe I might say, OK, why do I have to use a straight line? Why don't I sort of try to eyeball this so that I get as many blue points on the left side and as many brown crosses on the right side as I can? Maybe that's the best decision boundary. How do I know which is the best choice? Which of those things should I do? The rest of this lecture will talk about that, how to distinguish between these choices. But just to draw a slightly simpler example, maybe you have, there's some people who pay on time, there's some defaulters there, but maybe it's a little hard to distinguish between them exactly. And so I have a new point that I want to classify, this point here, and say, is that person going to default or is that person going to pay on time? Well, one way I could do this, answer this question, is I could look for their nearest neighbor, I could say, let's look at the training points, the blue and red ones, and find the training point that is closest to the query point, the green point, and assign the same label to the query point. So I'll say, OK, the nearest neighbor is blue, someone who paid on time, so I think that green point is going to pay on time. And if you do that, that's called a nearest neighbor classifier. Or another way I can do it is I could try to draw a decision boundary that sort of separates the red points from the blue points, maybe something like that. But if I do that, well, now the green point is on the right side of that line, along with a bunch of red points. So if I do this classifier, then I might say that that green point looks like it's going to default. So this is a different classifier. It's called a linear classifier. The linear classifier doesn't always make the same predictions as the nearest neighbor classifier. Which one is better? Well, let's think about that. By the way, we're going to learn lots of algorithms for learning linear classifiers over the next several weeks, but not today. So today, what we're going to think about is just what might make one type of classifier better than another. Why? How do you choose? And so let's look at a more complicated data set with the same two classifiers side by side. So there we go, nearest neighbor classifier on the left. And let me make clear, again, both these figures are the same data set, same blue point, same brown points. The only difference is what type of classifier we're using. And with these two classifiers, you'll see I've drawn some black lines. The black lines are called decision boundaries. Obviously, they're pretty different for the two classifiers, even though they're both potentially good classifiers. All right. So again, at left, we have a nearest neighbor classifier, which classifies a new point by finding the nearest point in the training data and assigning it the same class. And the way this figure works is if you look closely, you'll see there's some background color. I don't know if it's coming very well through on the projector. It's a little better in the lecture notes. But what you might notice is that in this region here, where all the blue points are, the background is also colored blue. And what the background colored blue means is that if you have a test point, a query point, that you want to predict the class of, and you find it somewhere in that blue region, we're going to say, OK, that's a good paying debtor. And if it's in the brown region, then we're going to say, oh, that's a defaulter. That's going to be our prediction. The prediction might be wrong, but that's how we predict things. And over on the right, well, the linear classifier, if you look closely, this region up here is colored brown, which means any test point that's found in this region, we're going to predict that it's a defaulter. Any test point that's down here in this blue region, we're going to predict will not default, will pay bills on time. One other subtle difference that probably isn't obvious here, but when we compute a linear decision boundary, this is a linear decision boundary, because it's a straight line. When we compute one, it's usually explicitly computed. We actually compute one. We actually compute one. We actually compute an equation for that line, for that decision boundary. With a nearest neighbor classifier on the other hand, we don't do that, because it's not very useful in practice. What we actually do is someone gives you a query point. You actually compute the distance from that query point to every training point. And you take the shortest distance and look at the label of that point, and that's what you return. So this is not explicitly computed, though we did explicitly compute it just to make this figure. It's an implicit thing that comes out of the nearest neighbor computation. So if you look at the two of these, and you ask, which do I think is better, one thing that's very clear is that the one on the left at least computes the correct answer for every training point. The one on the right doesn't even do that. The one on the right, there is a blue point right here, but it's on the brown side of the line. So if you feed that point into the linear classifier and say, please predict will that circle default or not? The linear classifier will predict that circle will default, which is wrong, or at least was wrong for the training point. So you might say, you can't even remember the training data that we used to teach you. How can that be a good classifier? Nevertheless, it is entirely possible that the one on the right is a much better classifier than the one on the left. Can someone tell me why? What is overfitting mean? OK, but why would it work well on the training data and not work well on the test data? Why? If you want to train data, say that it's a training data, say that it's a training data. Hmm. Oh, yeah. If it is, I want to do that. OK, that's a good insight. If there is noise in the training data, like if there are points that are just have the wrong label because of a data entry error, those wrong points are going to mess up your results. Did you have something you wanted to add? Ma'am. So if it is a problem, why would you use that boundary problem? Like, basically, that is like a problem. And if it is a problem, then you use that problem. Yeah, yeah. So that's kind of a just salt thing to say, but you're very right. Look at these. Look at these decision boundaries. Do these look like they would conform to some real insight about this data? Probably not. I mean, we can't say for sure. Well, for credit card debtors, we can say for sure. There's no way that those little bumps have meaning in credit card debt. Come on, there's no way. Those are just these are sort of outliers. They are points that, well, statistically, some people default, some people don't for reasons of their own, you're going to get points whose labels don't fit the general pattern. But they're giving you the wrong impression. If you think that most people who are up here are going to pay their credit card debt on time, you're wrong. There was that one outlying person who paid credit card debt on time. So when you see boundaries that are this sinewess, you really should question them. Because is that really fitting the true pattern underlying what makes people default or not default on their credit cards? Probably not. So sometimes you really need to smooth out the boundary in a way that looks at the big picture and doesn't get caught up in all the little details of what this one person did. That is something that linear decision boundaries are very good at is not getting caught up in too many small details. So yeah, I like how these three answers got closer and closer to the main point. I guess. If you have a point on the linear decision boundary, how do you go about classifying that? Usually it's just one arbitrary side is chosen. So maybe I'm going to say all of the points that are on the decision boundary, I'm going to give them a credit card. And we don't really have a statistical basis for making that decision. You just have to pick one and hope it doesn't happen too often. Orange shirt? Do we ever have a case where one model is not as good as being better than another model that we picked up a model that is less computationally accurate? Oh, yeah. Do we ever pick a worse model? Because it's less computationally intensive, absolutely. Sure. You can burn huge amounts of computation on doing really fancy neural network models, but you don't necessarily even have the computer to do that. Sometimes people use super computers to train these things. If you don't have one in your basement, then you do something cheaper. And of course, Blue shirt, did you have a question? Yeah, you, sir. I'm sorry, louder, please. Can we make this dynamic? I'm not aware of, well, how do you want to make a dynamic? Like add new training data as time goes by? I'm certain that there is research done on that, but we won't cover any in this class. So yeah, people think about that. Question over there, sir. So the question is, if we got rid of all those islands, but we still had the really awful bumpy shape for the, say, the main curve, is that also a reason not to trust that decision boundary? Have I paraphrased your question correctly? Yeah. Yes, that is a reason not to trust that decision boundary. The more bumpy and sinew as sin weird it is, the less I trust it. And so we'll see an intermediate solution in a moment. I'll take one more question from the back of the room. If you can talk loudly, please. I'll leave it on the other side. I'll leave it on the other side. Hmm. Well, you see, the question is if we had a really large amount of data, how would we know if this island is significant? And my main answer to that is if you get so much data that that island starts filling up with blue points, but it doesn't start filling up with brown crosses, then you start to know it's significant. But as you get more and more data, maybe you'll find lots of brown crosses in that little island, too. And then you should forget about it. All right, I want to move on. So sorry if there are more questions I missed. But here is another comparison of two classifiers. The classifier on the left is going to be exactly the same one we had before. So it's still the nearest neighbor classifier. But what I want to do now is compare the nearest neighbor classifier with what's called a 15 nearest neighbor classifier. So what the one on, by the way, this is still the same data set as before. Two more copies of the same data set. What the classifier on the right does is instead of looking for the nearest neighbor and checking its label, now you look for the 15 nearest neighbors, the 15 training points that are closest to your query point. And they vote. They cast 15 votes, and eight votes gets you a winning class. And so what you'll notice about this is now the decision boundary looks a lot smoother. That decision boundary looks like something that maybe you could kind of trust, maybe not every little tiny wobble in detail. But the overall picture looks a lot more like a pattern that might exist in the real world than the one nearest neighbor classifier does. So I'm just more likely to trust that. Yeah? Oh, no difference at all. Just to be totally clear, I am showing you actually the exact same figure twice with no changes whatsoever. All right. So when we look at this, all of these islands, all of these weird zigzags, we're pretty sure that this is an example of what's called overfitting, where the decision boundary is hewing too closely to the data and picking up a lot of spurious ideas about the classification that probably do not reflect the true underlying reality of the probability is involved in how people decide whether to pay their credit card debt. Whereas the one on the right, just because it's smoother, I think that one's more likely to correspond to a model of reality that we might have some trust in. All right. So a very brief excursion into what you'll do in homework one. You'll be working with a very famous data set called MNIST, which dates back up three or four decades when people wanted to figure out how to classify handwritten digits. So the data set you're going to get is going to have examples of handwritten digits by many, many different people. And there are 10 classes, the digits 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. So here are some typical sevens and some typical ones taken from this data set. We need to take these images and somehow present them as an input to our machine learning algorithm. Machine learning algorithms are used to receiving points, data points, which means appointing some high dimensional Euclidean space. How are we going to represent an image as a point in a high dimensional Euclidean space? Well, there's some pretty sophisticated ways you could do that, but we're going to start by doing it in the dumbest way possible. We're going to just express these images as vectors by taking all the pixel gray scale values and chaining them together in one big array. So you could imagine, for instance, that if these images were given to you as pixels and each pixel has a value of 0 for black, 3 for white, 1 or 2 for shades of gray, then maybe your handwritten digit, well, they're not 4 by 4, they're really 28 by 28, but pretend they're 4 by 4. Maybe it looks like this, and so what you do is you just write this out as one big vector of length 16. And we're going to think of this as a point in 16 dimensional space, 16 dimensional Euclidean space. And so a lot, there's a lot of geometry in machine learning, and so you're going to get need to get used to thinking of the data that you work with as being points in some high dimensional space. Now, when you have more than two dimensions, more than two variables, more than two pieces of information about each credit card holder, we can still have linear decision boundaries, but the decision boundary isn't a line anymore. It's a high dimensional, 15 dimensional thing called a hyperplane, and I will teach you all about hyperplane's next lecture. But this is what you need to know right now to do homework one, so there you go. What I want to spend the rest of the time in is something very important, also important for homework one. The main thing that I think we do right as machine learning people is that we make sure that our models that we train have statistical validity. And so I want to talk about the difference between training error, test error, and validation error, and also what is validation, and why is it the most super important part of machine learning. Here's how we do classification. First of all, we assume that we are given a bunch of labeled data. And labeled data means we're given sample points, points in say two dimensional space or 16 dimensional space or a million dimensional space. And each of those sample points comes with a class label. And as I said before, often we have two classes, but sometimes you have more. Maybe you want to distinguish whether your photo is a dog, a cat, or a human. That's a three class problem. So here's a really important part of what we do next. We're going to hold back a subset of this labeled data. We are not going to use all of the labeled data to train our classifier. We're going to hold some of them back. And the ones we hold back are called the validation set. And these are what we're going to use to make sure that our model that we train at the end has statistical validity of some sort. So maybe 20% of the points is typical, is what's often done. We'll be held back as the validation data. The other 80% of our labeled data is called the training set. And the training set is what we actually use to allow our classifier to learn something. I want to give you a big warning though. When you actually are out in the world of machine learning, you will find that people do not use the term training data in a consistent way. Often training data refers to all of the labeled data. In fact, even in homework one, which we're handed out, we say training data when we really mean all the labeled data. You're not going to use all of it to train. You're going to hold some of it back as a validation set. And so when someone says training data, you have to judge from context whether that's all the labeled data or just the training set. All right, now that we've held back 20% of our data, we are going to train one or more classifiers on the other 80% of our data on the training set. And so those classifiers are now doing what we call learning. They learn how to distinguish seven from not a seven, for instance, if you're doing the digit recognition problem. And you'd have a different classifier as to distinguish three from not three and so on. All right, and so we are using the training set to learn the weights in the model. Well, not all models have weights, but most of them have little numbers called weights that we need to figure out that we need to optimize and learn. That'll be a mess in for another day. The lesson for today is you do not use your validation data to train. All right, now usually what we're going to do, is either we're going to train multiple different learning algorithms, or more commonly, we're going to train multiple instances of one learning algorithm. But with multiple different hyperparameter settings, I'll say more about what a hyperparameter is in a moment, but learning algorithms often have knobs that you can turn to get different results and different classifiers. Or you might do both. You might train multiple different learning algorithms in each of them with multiple different hyperparameter settings. But the important thing is that you're using the same training set for each one of these different classifiers that you train. All right, so now you've got a bunch of classifiers and you want to know which one is the best one, and that's where we validate. You validate the trained classifiers on the validation set. And here's where we learned if our learning was actually worth anything. So essentially what this means is that you're going to take all your classifiers, you're going to ask them to make predictions on the validation set. You don't tell your classifier what the correct labels are, you ask your classifier, what are the correct labels, and you compare with the labels that you actually have, and now you know if that classifier is any good or not. So then we're going to choose the classifier or the hyperparameter settings or both that made the most correct predictions. We're going to put it another way that had the lowest error on the validation set. I will define validation error shortly, but this whole process of testing your classifier is to see if they're really good and picking the best one is called validation. Now again, I want to emphasize when we do validation, our classifiers are not learning anymore. All they're doing is they're taking in points that are in our validation set and making predictions on those points. We then check if they made the right predictions or the wrong predictions. We assign each classifier a score. We picked the best one. And so we're using this to judge our models. What we do not use to judge our models is how well they learned the training set. I mean a lookup table can learn the training set with 100% accuracy. That's not impressive. We need to test if these things can guess for new points. Now there's an additional step that's often called testing, but the set that is used to do it is called a test set. The test set again involves new data that was not in the original set of label data that you were given. You can think of this as being the final evaluation of your model. And the important thing here is that typically you do not have the labels. Some bought somebody else might have the labels and that somebody else might assign you a score. You trained a good classifier. You trained an awful classifier. Sometimes you do have the labels, but you hide them from yourself. You pretend you don't have the labels. You do the run your test and then you take the labels out of the vault and see how you did. Or maybe the labels are kept by a third party who verifies them for you. And that's what we're going to do in the homeworks with this website called Kaggle. By the way, just a quick aside, you've noticed I've been underlying words. Underlining words here. When I underline a word, that's a definition. You should memorize those definitions. That will help you do well in this class because you were expected to know the precise, rigorous meaning of those underlined words. They're not just random words. They're ones with strict definitions. All right, so I define three different kinds of sets. And accordingly, there are three different kinds of error called training error, validation error, and test error. So the training error is the fraction of the training set that is not classified correctly. As I remarked before, the training error is zero with the one nearest neighbor classifier, which makes the one nearest neighbor classifier sound very impressive if you're naive, but it's not impressive. I look like I said a lookup table can do that, but a lookup table can't generalize to new points. The training error is not zero with the 15 nearest neighbor classifier. It's not zero with the linear classifier, but often those are better classifiers predicting new data. And so one thing I want to say about training data is again, remember, the validation data does not go into this calculation. Even if somebody calls it training data, the validation set, we keep it out separately and we compute its own error and the error just on the validation data. So the validation error is the fraction of the validation set that is not classified correctly. And this is what we're going to use to choose our classifier. And if our classifier has hyper parameters that we have to set, we're going to use it to set our hyper parameters. Now, remember that you did not use the validation data to train your classifiers unless you're bad at this. So even the one nearest neighbor classifier can classify the validation set wrong. The validation error is almost always higher than the training error with almost any classification algorithm. That's okay, you should expect that. If the validation is error is lower, you should always be suspicious. Well, then there's the test error, which is the fraction of the test set, not classified correctly. And the test error is used to evaluate you. Do you know how to construct a good classifier? All right, let me come back to hyper parameters. Most ML algorithms have at least one or two parameters that you can adjust to get different results, especially maybe smoother or less smooth decision boundaries. So often, but not always, the things that we call hyper parameters control a trade-off between overfitting and underfitting. And one example of a hyper parameter is the parameter k in the k nearest neighbor classifier. So let me show you a chart, a graph, that comes out of one of your textbooks from Elements of Statistical Learning. And it's pretty useful for showing what happens with nearest neighbors. All right, so this chart's a little bit weird, a little hard to understand because there's so much going on to take in first. The first thing I want you to do is look at the x-axis, which is k. So k is, I'm going to take a query point that I want to classify. I look for the k points in the training set that are closest to this query point. And they've done something very weird here, they have k going up from right to left. So here's what happens when you have one nearest neighbor. Here's what happens when you have three nearest neighbors. Here's what happens when you have five nearest neighbors. Why did they do this? Well, we often like to plot these graphs in machine learning so that underfitting is on the left hand side and overfitting is on the right hand side. So that's what they did. And I know it's a little confusing, but understand that k is one at the right and then goes up to 151 nearest neighbors at the left. So that's the x-axis. What is the y-axis now? Well, there are two lines graphed here. You can see that there's a blue line, which is the training error. You can see that there's an orange line. The orange line, you can think of it as either the validation error or the test error. It doesn't really matter. They both behave the same way usually. And what you'll notice about the training error is that, well, as we, when you have one nearest neighbor, the training error is zero, as I said before, one nearest neighbor always classifies all the training points correctly. So the training error is zero, which looks great, but no, no, no, no. As you increase k, the training error gets worse and worse. When you get to the extreme of 151, well, you're just making random choices. That's not good. But what's interesting here is that the test error tells a different story than the training error. What the test error says is that we have a ushaped curve that the best choice is not the far left or the far right. The best choice is seven nearest neighbors right here. That's what gives us the lowest test error. If we go to one nearest neighbor, well, look, we're overfitting. Our test error actually starts getting worse as k goes below seven, because we're getting these sinuous boundaries that don't mean anything that aren't correct. But if we go to the other extreme, if we make k too large, then now we're underfitting. We are smoothing out the boundaries so much that the decision boundary no longer means anything. So for many machine learning algorithms, we will find that there's a happy medium between overfitting and underfitting, and that's what we're looking for. By the way, neural networks are a famous exception to this rule. But most of the algorithms we study this semester will have this property that we're on a tight rope between underfitting and overfitting. So let me just define those terms. What do I mean when I say overfitting or underfitting? Well, these aren't totally rigorous definitions, but they're phenomena that we often see in practice, and we will have some statistical ways of understanding these a little bit later in the semester around lecture 12. But as concepts, these are great. These are really good things to understand. To see why your data acts the way it does. Why your classifiers act the way they do. Overfitting is when the validation error or the test error deteriorates because the classifiers just becoming too sensitive to spurious patterns or outliers. I'm going to define outliers in a minute too. Underfitting, well, it's at the other end of the graph, so it's sort of the opposite, but it's not really an opposite. It is when the validation and test error also deteriorates for a completely different reason, because now your classifier is just not flexible to fit the patterns that you are trying to fit. And one of the main causes of overfitting is outliers. Outliers are points with atypical labels. Now, the extreme version of an atypical label is a data entry error where somebody just entered the wrong class into the computer, so you've got a point totally in the wrong place. But you have to actually worry about statistical outliers too, which may be real, correct data, but just representing the vagaries of human nature. So maybe a borrower is very, very rich, but defaulted anyway. Why we don't know. And the important point about outliers is that the more you have, the more your risk of overfitting, and the more you need a method that will smooth out your decision boundaries a little bit. Because in machine learning, the goal is to create a classifier that generalizes to new data that we haven't seen yet. And so overfitting and underfitting are both counterproductive to that goal in different ways. We're always seeking a compromise. We want decision boundaries that make fine distinctions, but aren't downright superstitious. Last thing I want to mention is that for homework one, the way that we will be running your test set is on a website called Kaggle.com, which is designed to run machine learning competitions, including our homeworks. And so you will have no access to the labels of the test set, but Kaggle will and Kaggle will assign you a score. Thanks very much and on next Monday we'll talk about decision boundaries and linear classifiers some more. Thank you.